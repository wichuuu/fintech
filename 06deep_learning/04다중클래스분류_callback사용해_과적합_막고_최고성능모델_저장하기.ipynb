{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1435e4b9",
   "metadata": {},
   "source": [
    "# 04다중클래스분류_callback사용해_과적합_막고_최고성능모델_저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d91db7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import koreanize_matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bda36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/winequality-white.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7808277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.00100</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.99400</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.99510</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4898 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.0              0.27         0.36            20.7      0.045   \n",
       "1               6.3              0.30         0.34             1.6      0.049   \n",
       "2               8.1              0.28         0.40             6.9      0.050   \n",
       "3               7.2              0.23         0.32             8.5      0.058   \n",
       "4               7.2              0.23         0.32             8.5      0.058   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "4893            6.2              0.21         0.29             1.6      0.039   \n",
       "4894            6.6              0.32         0.36             8.0      0.047   \n",
       "4895            6.5              0.24         0.19             1.2      0.041   \n",
       "4896            5.5              0.29         0.30             1.1      0.022   \n",
       "4897            6.0              0.21         0.38             0.8      0.020   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    45.0                 170.0  1.00100  3.00       0.45   \n",
       "1                    14.0                 132.0  0.99400  3.30       0.49   \n",
       "2                    30.0                  97.0  0.99510  3.26       0.44   \n",
       "3                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "4                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "4893                 24.0                  92.0  0.99114  3.27       0.50   \n",
       "4894                 57.0                 168.0  0.99490  3.15       0.46   \n",
       "4895                 30.0                 111.0  0.99254  2.99       0.46   \n",
       "4896                 20.0                 110.0  0.98869  3.34       0.38   \n",
       "4897                 22.0                  98.0  0.98941  3.26       0.32   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         8.8        6  \n",
       "1         9.5        6  \n",
       "2        10.1        6  \n",
       "3         9.9        6  \n",
       "4         9.9        6  \n",
       "...       ...      ...  \n",
       "4893     11.2        6  \n",
       "4894      9.6        5  \n",
       "4895      9.4        6  \n",
       "4896     12.8        7  \n",
       "4897     11.8        6  \n",
       "\n",
       "[4898 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db51fd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4898 entries, 0 to 4897\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         4898 non-null   float64\n",
      " 1   volatile acidity      4898 non-null   float64\n",
      " 2   citric acid           4898 non-null   float64\n",
      " 3   residual sugar        4898 non-null   float64\n",
      " 4   chlorides             4898 non-null   float64\n",
      " 5   free sulfur dioxide   4898 non-null   float64\n",
      " 6   total sulfur dioxide  4898 non-null   float64\n",
      " 7   density               4898 non-null   float64\n",
      " 8   pH                    4898 non-null   float64\n",
      " 9   sulphates             4898 non-null   float64\n",
      " 10  alcohol               4898 non-null   float64\n",
      " 11  quality               4898 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 459.3 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8614a4ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAGcCAYAAAACtQD2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAABa10lEQVR4nO3deVwV1f8/8Bf3souAorKjbCqbXHAFE8qlcs36aJq4ZKGCubRoaYpgrqVpWrhli4lmtmvR4lJSaKYZdkVNMwo3UElAZb/3/P7gd+fLZZNN73B9PR8PHnpnzsy878zcmfecOXPGRAghQERERCQTCkMHQERERFQRkxMiIiKSFSYnREREJCtMToiIiEhWmJwQERGRrDA5ISIiIllhckJERESywuSEiIiIZMXU0AHUl1arxaVLl9CyZUuYmJgYOhwiIiKqAyEEbty4ARcXFygUtdeNNLvk5NKlS3B3dzd0GERERNQA58+fh5ubW61lml1y0rJlSwDlX87W1tbA0RAREVFd5Ofnw93dXTqP16bZJSe6Wzm2trZMToiIiJqZujTJYINYIiIikhUmJ0RERCQrTE6IiIhIVpicEBERkawwOSEiIiJZYXJCREREssLkhIiIiGSFyQkRERHJSrPrhI2IqK40Gg3UajVycnLg4OCAoKAgKJVKQ4dFRLfB5ISIjFJKSgrWr1+PrKwsaZiTkxNiY2MRERFhwMiI6HZ4W4eIjE5KSgoSEhLg5eWFxMREJCcnIzExEV5eXkhISEBKSoqhQySiWpgIIYShg6iP/Px82NnZIS8vj+/WIaIqNBoNxo4dCy8vLyxatEjv1exarRZxcXHIyMjA1q1beYuH6C6qz/mbNSdEZFTUajWysrIQFRWll5gAgEKhwJgxY3D58mWo1WoDRUhEt8PkhIiMSk5ODgDA09Oz2vG64bpyRCQ/TE6IyKg4ODgAADIyMqodrxuuK0dE8sPkhIiMSlBQEJycnLBt2zZotVq9cVqtFtu3b4ezszOCgoIMFCER3Q6TEyIyKkqlErGxsTh06BDi4uKQnp6OgoICpKenIy4uDocOHUJMTAwbwxLJGJ/WISKjVF0/J87OzoiJiWE/J0QGUJ/zN5MTIjJa7CGWSD7qc/5mD7FEZLSUSiVUKpWhwyCiemKbEyIiIpIVJidEREQkK0xOiIiISFaYnBAREZGsMDkhIiIiWWFyQkRERLLC5ISIiIhkhckJERERyQqTEyIiIpIVJidEREQkK0xOiIiISFaYnBAREZGsMDkhIiIiWWFyQkRERLLC5ISIiIhkhckJERERyQqTEyIiIpIVJidEREQkK0xOiIiISFYalZycPn0aFhYWWLhwoTTs8uXLGDx4MIKDgxEUFIQNGzboTSOEwOLFixEQEIDAwEA88cQTyM/Pb0wYREREZEQalZzMnDkTffv2RWlpqTTsf//7H8aMGYPjx4/j4MGDeP/995GcnCyN37RpE3755RccO3YMJ06cQEhICKKjoxsTBhERERmRBicnn376KRwdHdGzZ09p2B9//AGNRoOoqCgAQMuWLfHKK69g06ZNUpmNGzdixYoVsLCwAADMmjULhw8fRk5OTkNDISIiIiPSoOSkoKAACxYswPLly/WG7927F5GRkXrD+vTpg/3790MIgZycHFy8eBF+fn7/F4BCgfDwcOzfv78hoRAREZGRMW3IREuXLkVUVBRcXFz0hl+6dAnt27fXG2ZlZQVLS0tcuXIFV69ehZubW5X5ubu74++//652WcXFxSguLpY+s30KERGRcat3zcm5c+fw6aef4vnnn68yLjc3F5aWllWGW1paoqCg4Lbjq7Ns2TLY2dlJf+7u7vUNmYiIiJqReicnM2fOxOLFi6tNMiwsLFBUVFRleGFhIaysrG47vjpz585FXl6e9Hf+/Pn6hkxERETNSL1u63z77bcoKCjA//73v2rHu7m5ITMzU29YYWEhbt68iXbt2kEIUWU8AJw/fx7BwcHVztPCwkJqPEtERETGr17JSUZGBi5cuACVSiUNy8rKAlCeuLz++uuYPXu23jQpKSno3r07FAoFnJ2dYWNjg5MnT8Lf3x8AoNVq8fPPP2Pp0qWN/CpERERkDOp1Wyc2NhZnzpxBWlqa9BcTE4Po6GgcPXoUERERKC0txbZt2wAAN27cQHx8PKZPny7NY8aMGXjxxRdRUlICAFi5ciWCg4Ph5eXVhF+LiIiImqsGPa1TkZmZGUxMTAAAJiYm+OKLLzB58mQsX74cGo0G0dHRGDlypFT+2WefRU5ODoKDg6FQKODn54ctW7Y0NgwiIiIyEiZCCGHoIOojPz8fdnZ2yMvLg62traHDISIiojqoz/mbL/4jIiIiWWFyQkRERLLC5ISIiIhkhckJERERyQqTEyIiIpIVJidEREQkK0xOiIiISFaYnBAREZGsMDkhIiIiWWFyQkRERLLC5ISIiIhkhckJERERyQqTEyIiIpIVJidEREQkK6aGDoCI6E7RaDRQq9XIycmBg4MDgoKCoFQqDR0WEd0GkxMiMkopKSlYv349srKypGFOTk6IjY1FRESEASMjotvhbR0iMjopKSlISEiAl5cXEhMTkZycjMTERHh5eSEhIQEpKSmGDpGIamEihBCGDqI+8vPzYWdnh7y8PNja2ho6HCKSGY1Gg7Fjx8LLywuLFi2CQvF/12BarRZxcXHIyMjA1q1beYuH6C6qz/mbNSdEZFTUajWysrIQFRWll5gAgEKhwJgxY3D58mWo1WoDRUhEt8PkhIiMSk5ODgDA09Oz2vG64bpyRCQ/TE6IyKg4ODgAADIyMqodrxuuK0dE8sPkhIiMSlBQEJycnLBt2zZotVq9cVqtFtu3b4ezszOCgoIMFCER3Q6TEyIyKkqlErGxsTh06BDi4uKQnp6OgoICpKenIy4uDocOHUJMTAwbwxLJGJ/WISKjVF0/J87OzoiJiWE/J0QGUJ/zN5MTIjJa7CGWSD7qc/5mD7FEZLSUSiVUKpWhwyCiemKbEyIiIpIVJidEREQkK0xOiIiISFaYnBAREZGsMDkhIiIiWeHTOkRktPgoMVHzxOSEiIxSdZ2wOTk5ITY2lp2wEckcb+sQkdFJSUlBQkICvLy8kJiYiOTkZCQmJsLLywsJCQlISUkxdIhEVAv2EEtERkWj0WDs2LHw8vLCokWLoFD83zWYVqtFXFwcMjIysHXrVt7iIbqL6nP+Zs0JERkVtVqNrKwsREVF6SUmAKBQKDBmzBhcvnwZarXaQBES0e0wOSEio5KTkwMA8PT0rHa8briuHBHJD5MTIjIqDg4OAICMjIxqx+uG68oRkfwwOSEioxIUFAQnJyds27YNWq1Wb5xWq8X27dvh7OyMoKAgA0VIRLfD5ISIjIpSqURsbCwOHTqEuLg4pKeno6CgAOnp6YiLi8OhQ4cQExPDxrBEMsandYjIKFXXz4mzszNiYmLYzwmRAdTn/M3khIiMFnuIJZKP+py/2UMsERktpVIJlUpl6DCIqJ7Y5oSIiIhkhckJERERyQqTEyIiIpIVJidEREQkK0xOiIiISFaYnBAREZGsMDkhIiIiWWFyQkRERLLC5ISIiIhkhckJERERyQqTEyIiIpIVvluHiIwWX/xH1DwxOSEio5SSkoL169cjKytLGubk5ITY2FhEREQYMDIiuh3e1iEio5OSkoKEhAR4eXkhMTERycnJSExMhJeXFxISEpCSkmLoEImoFiZCCGHoIOojPz8fdnZ2yMvLg62traHDISKZ0Wg0GDt2LLy8vLBo0SIoFP93DabVahEXF4eMjAxs3bqVt3iI7qL6nL9Zc0JERkWtViMrKwtRUVF6iQkAKBQKjBkzBpcvX4ZarTZQhER0O2xzQkRGJScnBwDg6elZbYNYT09PvXJEJD9MTojIqDg4OAAAPv/8c+zevbtKg9ghQ4bolSMi+WFyQkRGJSgoCPb29nj77bcRFhaGuLg4eHp6IiMjA0lJSdi8eTPs7e0RFBRk6FCJqAZMTojIaAkh8Oeff+Kff/5BcXExdO3/TUxMDBwZEdWGyQkRGRW1Wo3c3Fz069cPP/74I3755RdpnFKpRL9+/bBv3z6o1WqoVCrDBUpENWJyQkRGRdfQdf/+/ejVqxd69OgBCwsLFBcX49dff8X+/fv1yhGR/NT7UeK1a9eiS5cuCA4ORufOnTFu3DhcvHhRGn/q1ClERkZCpVIhJCQEn332md70paWlmDlzJgICAhAQEIDp06ejpKSk8d+EiAhAq1atAACBgYFYvHgxhg8fjoEDB2L48OFYvHgxAgIC9MoRkfzUOzkZOnQofv31Vxw/fhwnTpxAhw4dpNbvRUVFeOSRR/DKK68gLS0N33zzDebOnYs//vhDmn7BggUoLi6GWq2GWq2GEALz589vum9ERPe02/UrqWtv0sz6nyS6p9Q7OfH09ISlpSUAwNTUFAsXLsTff/+NS5cu4fvvv0dISAgiIyMBlD+298ILL+Ddd98FUN4749atW7F8+XIoFAooFAosXboU27Ztg0ajacKvRUT3qtzcXADlbU/i4uKQnp6OgoICpKenIy4uTup8TVeOiOSn0T3EFhQUwMTEBA4ODti7d6+UmOhERkZiz549AIC0tDS4uLjA3t5eGm9rawsPDw8cO3as2vkXFxcjPz9f74+IqCa6/kuio6Px999/Y9q0aRg8eDCmTZuGjIwMREdH65UjIvlpVHKSnp6OUaNGIT4+HhYWFrh06RLc3d31yri7u+Pvv/8GgGrHVy5T2bJly2BnZyf9VTc9EZFOUFAQnJyccPLkSbz33nt45plnMHz4cDzzzDN49913cfLkSTg7O7OfEyIZa9DTOrNnz8bWrVuRnZ2N6OhozJw5E0B5Nanulo+OpaUlioqKIISodryuTEFBQbXLmjt3Lp5//nnpc35+PhMUIqqRUqlEbGws4uPjMXz4cBQXF0vjNm/ejOLiYixcuJAv/SOSsQbVnKxYsQJZWVm4du0aLC0tMXHiRACAhYUFioqK9MoWFhbCwsICJiYm1Y7XlbGysqp2WRYWFrC1tdX7IyK6nZo6WmMHbETy16h+ThwcHLBmzRrY29tj7dq1cHNzQ2Zmpl6Z8+fPw83NDQCqHV+5DBFRY2g0Gqxfvx5hYWFISEhAenq69OK/gIAAJCQkYMOGDejduzdrT4hkqtENYouLi1FSUgKNRoPw8HAcOHBAb/yBAwcQHh4OAFCpVDh79qxeK/n8/HycPn0aoaGhjQ2FiO4SjUaDtLQ07Nu3D2lpabJ62k6tViMrKwtRUVEwMzODSqVCv379oFKpYGZmhjFjxuDy5cvSUztEJD/1qjkpKSnBlStXpFqO3NxcTJ48GSNGjEDr1q0xYsQILFiwAAcOHEBkZCSysrKwcuVKJCUlAQCsrKwwYcIEzJkzB+vWrYOJiQlefvllREVFwdrauum/HRE1uZSUFKxfv77K235jY2MRERFhwMjK6Xp+9fT0rHa8bjh7iCWSr3rVnFy9ehWPPPIIOnfuDJVKhb59+6JHjx547733AAAtWrTArl278PLLLyM4OBgDBgzAwoUL0bNnT2ker776KgAgICAA/v7+KC4uxsqVK5vwKxHRnZKSkoKEhAR4eXkhMTERycnJSExMhJeXFxISEpCSkmLoEKVHhDMyMqodrxvOR4mJ5MtENLNuEvPz82FnZ4e8vDw2jiW6izQaDcaOHQsvLy8sWrQICsX/XdtotVrExcUhIyMDW7duNWhbjuYSJ9G9pj7n70a3OSGie0PFthwVT/gAoFAoZNOWQ/co8aFDh6rtIfbQoUOIiYlhYkIkY3wrMRHVSXNqyxEREYGEhASsX78e06ZNk4Y7OTkhISFBFm1jiKhmrDkhojppjm05Kt+1bmZ3sYnuWUxOiKhOdN3Cb9u2DVqtVm+cVqvF9u3bZdMtvK7hrre3t17DXW9vb9k03CWimjE5IaI6aS5tOSp2wrZo0SL4+/vDysoK/v7+WLRoEcLCwrBhwwZZ9c1CRPqYnBBRnenaclT3tl+5tOVoLg13iahmbBBLRPUSERGB3r17Q61WS93CBwUFGbzGRKc5NdwlouoxOSGielMqlVCpVIYOo1oVG+76+/tXGS/HhrtEpI+3dYjIqDSnhrtEVD3WnBCRUdE13E1ISMC8efPg6uqK4uJiWFhY4OLFizh8+DASEhJkcxuKiKpickJERiciIgLh4eFITU2tMq53796yaLhLRDVjckJERmfDhg1ITU1Fq1atMGDAALi4uODSpUvYs2cPUlNTsWHDBsTExBg6TCKqAV/8R0RGpaSkBIMGDYKtrS127twJU9P/uwYrKyvD448/jvz8fCQnJ8Pc3NyAkRLdW/jiPyK6Z+3atQsajQZPP/20XmICAKamppg4cSI0Gg127dploAiJ6HaYnBCRUbl48SIAICwsrNrxuuG6ckQkP0xOiMiouLq6AgAOHTpU7XjdcF05IpIfJidEZFSGDRsGpVKJd955B2VlZXrjysrK8N5770GpVGLYsGEGipCIbofJCREZFXNzc4wYMQLXr1/H448/jt27d+PatWvYvXs3Hn/8cVy/fh0jRoxgY1giGeOjxERkdHSPCX/88cdYtWqVNFypVGLUqFF8jJhI5lhzQkRGyd/fH23atNEb5uDgUO37dohIXpicEJHRSUlJQXx8PPLy8vSG5+XlIT4+HikpKQaKjIjqgrd1iMioaDQarF69GgCgUqng5uYmvVvnwoULOHz4MFavXo3evXvz/TpEMsXkhIiMyvHjx5Gbm4s2bdrgyJEjOHz4sDROoVCgTZs2uHbtGo4fP47Q0FADRkpENeFtHSIyKr///jsA4Nq1a7Czs8OsWbPw6aefYtasWbCzs8O1a9f0yhGR/LDmhIiMilarBQDY2NjovVtn8ODBeOihh/Doo4/i5s2bUjkikh/WnBCRUblx4wYAwM7ODgqF/iFOoVBILxzTlSMi+WHNCRE1W0VFRcjMzNQblp+fD6D83TnPPvssBg4cCFdXV1y8eBHffPMNLl26JJU7c+aMNJ2HhwcsLS3vXvBEVCMTIYQwdBD1UZ9XLhORcTtz5gymTJnSJPPauHEjOnbs2CTzIqKq6nP+Zs0JETVbHh4e2Lhxo96wsrIyTJ8+Hebm5rC2tsZ///0njXNwcMCtW7dQUlKCN998U2qPopsXEckDkxMiarYsLS2rre0YOXIkPvroI1haWqJ///7Yu3cv+vfvj6NHj6KoqAijRo1iT7FEMsbbOkRklDZs2IBPPvkEGo1GGqZUKjFixAi+W4fIAOpz/mZyQkRGq6SkBJs3b8bHH3+MkSNHIjo6mm8jJjKQ+py/+SgxERktc3Nz9O/fHwDQv39/JiZEzQSTEyIiIpIVJidEREQkK0xOiIiISFaYnBAREZGsMDkhIiIiWWFyQkRERLLC5ISIiIhkhckJERERyQqTEyIiIpIVJidEREQkK0xOiIiISFaYnBAREZGsMDkhIiIiWWFyQkRERLLC5ISIiIhkhckJERERyQqTEyIiIpIVJidEREQkK0xOiIiISFaYnBAREZGsMDkhIiIiWWFyQkRERLLC5ISIiIhkhckJERERyQqTEyIiIpIVJidEREQkK0xOiIiISFaYnBAREZGsMDkhIiIiWWFyQkRERLLC5ISIiIhkpd7JSXJyMvr164cuXbogMDAQMTExKCgokMafOnUKkZGRUKlUCAkJwWeffaY3fWlpKWbOnImAgAAEBARg+vTpKCkpafw3ISIiIqNQ7+TExsYGH3zwAf744w+kpaXhxo0bWLBgAQCgqKgIjzzyCF555RWkpaXhm2++wdy5c/HHH39I0y9YsADFxcVQq9VQq9UQQmD+/PlN942IiIioWat3chIREQFXV1cAgKmpKWbPno3vv/8eAPD9998jJCQEkZGRAAAnJye88MILePfddwEAWq0WW7duxfLly6FQKKBQKLB06VJs27YNGo2mqb4TERERNWONbnPy33//wdLSEgCwd+9eKTHRiYyMxJ49ewAAaWlpcHFxgb29vTTe1tYWHh4eOHbsWGNDISIiIiPQ6ORkw4YNGD9+PADg0qVLcHd31xvv7u6Ov//+u8bxlctUVlxcjPz8fL0/IiIiMl6NSk6+++47pKWlYdKkSQCA3NxcqRZFx9LSEkVFRRBCVDteV6Zio9qKli1bBjs7O+mvuuSGiIiIjEeDk5Pz589j8uTJ2L59OywsLAAAFhYWKCoq0itXWFgICwsLmJiYVDteV8bKyqra5cydOxd5eXnS3/nz5xsaMhERETUDpg2Z6NatWxg+fDgWL16Mbt26ScPd3NyQmZmpV/b8+fNwc3OrcXzlMpVZWFhIyQ8REREZv3rXnGg0GowePRoDBw7EuHHj9MaFh4fjwIEDesMOHDiA8PBwAIBKpcLZs2eRm5srjc/Pz8fp06cRGhragPCJiIjI2NQ7OZk5cyasrKywaNGiKuNGjBiBw4cPSwlKVlYWVq5ciWeeeQYAYGVlhQkTJmDOnDnQarUQQuDll19GVFQUrK2tG/lViIiIyBjU67bO9evXkZiYiE6dOiEkJEQabmJigm+//RaOjo7YtWsXpk6dips3b0Kr1WLhwoXo2bOnVPbVV1/Fs88+i4CAAADAfffdh7Vr1zbR1yEiIqLmrl7JSatWrSCEqLVMcHAwUlNTaxxvaWmJDRs21GexREREdA/hi/+IiIhIVpicEBERkawwOSEiIiJZYXJCREREssLkhIiIiGSFyQkRERHJCpMTIiIikhUmJ0RERCQrTE6IiIhIVpicEBERkawwOSEiIiJZYXJCREREssLkhIiIiGSFyQkRERHJCpMTIiIikhUmJ0RERCQrTE6IiIhIVkwNHQARUW2ys7ORl5fX4OkzMzP1/m0IOzs7ODo6Nnh6IqofEyGEMHQQ9ZGfnw87Ozvk5eXB1tbW0OEQ0R2UnZ2N8RMmoKS42KBxmFtY4IMtW5igEDVCfc7frDkhItnKy8tDSXExlPf1hImdYS5GRF4+Sn4+jLy8PCYnRHcJkxMikj0TO1uYOLQ2dBhEdJewQSwRERHJCpMTIiIikhUmJ0RERCQrTE6IiIhIVpicEBERkawwOSEiIiJZYXJCREREssLkhIiIiGSFnbARUb1pNBqo1Wrk5OTAwcEBQUFBUCqVhg6LiIwEkxMiqpeUlBSsX78eWVlZ0jAnJyfExsYiIiLCgJERkbHgbR0iqrOUlBQkJCTAy8sLiYmJSE5ORmJiIry8vJCQkICUlBRDh0hERoDJCRHViUajwfr16xEWFoZFixbB398fVlZW8Pf3x6JFixAWFoYNGzZAo9EYOlQiauaYnBBRnajVamRlZSEqKgoKhf6hQ6FQYMyYMbh8+TLUarWBIiQiY8HkhIjqJCcnBwDg6elZ7XjdcF05IqKGYoNYIqoTBwcHAEBGRgY6depU5WmdjIwMvXJERA3F5ISI6iQoKAhOTk5Yu3Yt8vLyqjytY2dnB2dnZwQFBRkwSiIyBrytQ0R1olQqERkZiT///BPFxcV44YUX8Mknn+CFF15AcXEx/vzzT0RERLC/EyJqNNacEFGdaDQaHDhwAJ06dUJubi5ef/11aZyTkxM6deqElJQUTJo0iQkKETUKkxMiqhPd0zpxcXHVtjk5ffo0pk2bBrVaDZVKZehwiagZ420dIqoTPq1DRHcLa06IqE50T+F89tln+Oqrr6o0iB0yZIheOSKihmJyQkR1EhQUBHt7e2zevBlhYWGIi4uDp6cnMjIykJSUhM2bN6NVq1Z35GkdkZff5PNsDssmulcxOSGiehNC4MyZM/jnn39QUlICIYQ0/E7Q/Hz4jsyXiOSJyQkR1YlarUZubi769++PH374Ab/88os0TqlUol+/fti3b98daRCrvK8nTOxsm3SedSXy8pkcEd1lTE6IqE50DV337duHXr16oUePHrCwsEBxcTF+/fVX7N+/X69cUzKxs4WJQ+smny8RyROf1iGiOmnVqhUAIDAwEAsXLkSHDh1gbm6ODh06YOHChQgICNArR0TUUKw5IaI60bUnycvLw7hx45CdnS2Nc3R0hIWFhV45IqKGYnJCRHWSm5sLAMjMzIRCoV/pevXqVWi1Wr1yREQNxds6RFQnFW/XmJrqX9dU/MzbOkTUWKw5IaI60Wg0AICWLVtix44dSE5OxsWLF+Hq6opBgwZh9OjRuHHjhlSOiKihmJwQUZ2o1WoAwI0bN/DYY4+huLhYGrd582bps1qtRvfu3Q0SIxEZB97WIaI6qWtDVzaIJaLGYs0JEdVJcHAwkpKS0LJlS3z88cc4deqU9FZiPz8/jBw5Ejdu3EBwcLChQyWiZo41J0RUJyYmJgDKb+ssXLgQZmZmCAsLg5mZGRYuXIgbN27olSMiaijWnBBRnVR8RPi3337DoUOHpM/m5ubVliMiagjWnBBRnTg4OAAA+vXrh7KyMr1xZWVl6Nevn145IqKGYnJCRHUSFBQEe3t77Nu3r9p+Tvbt2wd7e3sEBQUZKEIiMha8rUNEdVZaWgoAsLCwgJ+fH4QQMDExwd9//42SkhJpPBFRYzA5IaI6SUtLw61bt2BpaYkbN27g+PHjeuMtLS1x69YtpKWloWvXrgaKkoiMAW/rEFGd6JKRoqIimJqawtfXFwEBAfD19YWpqSmKior0yhERNRRrToioTkpKSgCUPypcVlaGs2fP6o03MTGBEEIqR0TUUExOiKhOLly4AKC8B1h7e3tER0cjLCwMhw4dwubNm6VHiHXlmpLIy2/yeTaHZRPdq5icEFGdFBQUSP/v2LEjMjIycOrUKVhaWqJjx4749ddfq5RrLDs7O5hbWKDk58NNNs+GMLewgJ2dnUFjILqXNDg5effddxEbG4s///wTHTp0kIafOnUKMTExyMvLg4mJCeLi4vDYY49J40tLSzFr1izs3bsXANC3b1+8/vrrep04EZH8VHwS59dff5WSkdrKNZajoyM+2LIFeXl5DZ5HZmYmlixZgnnz5sHDw6NB87Czs4Ojo2ODYyCi+mlQchIXF4ejR4+iVatWep0xFRUV4ZFHHsHbb7+NyMhIZGVlITIyEj4+PujSpQsAYMGCBSguLpbecDpjxgzMnz8fr732WhN8HSK6Uzw9PXHixIk6lWtKjo6OTZIYeHh4oGPHjk0QERHdafV+Wker1cLZ2RlfffUVLC0t9cZ9//33CAkJQWRkJADAyckJL7zwAt59911p2q1bt2L58uVQKBRQKBRYunQptm3bBo1G0wRfh4juFBcXF+n/CoUCDg4OaN26NRwcHKBQKKotR0TUEPVOThQKBaZOnQqlUlll3N69e6XERCcyMhJ79uwBUN5PgouLC+zt7aXxtra28PDwwLFjx+obChEZiFarRU5ODv777z/k5ORAq9UaOiQiMiJN2s/JpUuX4O7urjfM3d0df//9d43jK5eprLi4GPn5+Xp/RHT3ZWdnN2k5IqKaNGlykpubW+VWj6WlJYqKiiCEqHa8rkxNLfyXLVsGOzs76a+65IaI7ry6vtCPL/4josZq0uTEwsJC6iVSp7CwEBYWFjAxMal2vK6MlZVVtfOcO3cu8vLypL/z5883ZchEVEe6RuwA9NqYVP5csRwRUUM0aT8nbm5uyMzM1Bt2/vx5uLm51Ti+cpnKLCwsYGFh0ZRhElEDnDp1Svq/UqnEqFGjMGjQICQnJ+OTTz6R2p1ULEdE1BBNWnMSHh6OAwcO6A07cOAAwsPDAQAqlQpnz56VepIEgPz8fJw+fRqhoaFNGQoRNTHdE3VmZmbQarX48MMPMW7cOHz44YfQarUwNTXVK0dE1FBNmpyMGDEChw8flhKUrKwsrFy5Es888wwAwMrKChMmTMCcOXOg1WohhMDLL7+MqKgoWFtbN2UoRNTEnJycAJR3srZjxw707t0bnp6e6N27N3bs2CH1eaQrR0TUUI26rWNubg4zMzPpc4sWLbBr1y5MnToVN2/ehFarxcKFC9GzZ0+pzKuvvopnn30WAQEBAID77rsPa9eubUwYRHQX9OjRQ3qqbuTIkdLwjIwMpKam6pUjImqMRiUnZ86cqTIsODhY70BVmaWlJTZs2NCYxRKRAXTv3h07duyoUzkiosZo0ts6RGS8/Pz8mrQcEVFNmJwQUZ18+umnTVqOiKgmTE6IqE6++uqrJi1HRFQTJidEVCd1fXUEXzFBRI3F5ISI6sTExKRJyxER1YTJCRHVSeXO1Vq3bo25c+eidevWtZYjIqqvJu2+noiMV3Fxsd7n//77D8uWLbttOSKi+mJyQkTVKioqqvZdWHVRuQ8kDw+Pat9ITkRUHSYnRFStzMxMTJkypUHTVp5u48aN6NixY1OERUT3ACYnRFQtDw8PbNy4Ufo8Z84cXL9+/bbTtWrVCsuXL68yLyKiumJyQkTVsrS01Kvt2LhxIx5//PHbTrdx40a0bdv2ToZGREaOT+sQUZ20bdsWNjY2tZaxsbFhYkJEjcbkhIjqbPfu3TUmKDY2Nti9e/ddjoiIjBGTEyKql927d2Pnzp1wcHAAADg4OGDnzp1MTIioyTA5oXuKRqNBWloa9u3bh7S0NHYY1kBt27bF0qVLAQBLly7lrRwialJsEEv3jJSUFKxfvx5ZWVnSMCcnJ8TGxiIiIsKAkRERUUWsOaF7QkpKChISEuDl5YXExEQkJycjMTERXl5eSEhIQEpKiqFDJCKi/4/JCRk9jUaD9evXIywsDIsWLYK/vz+srKzg7++PRYsWISwsDBs2bOAtHiIimWByQkZPrVYjKysLUVFRUCj0d3mFQoExY8bg8uXLUKvVBoqQiIgqYnJCRi8nJwcA4OnpWe143XBdOSIiMiwmJ2T0dI+8ZmRkVDteN1xXjoiIDIvJCRm9oKAgODk5Ydu2bdBqtXrjtFottm/fDmdnZwQFBRkoQiIiqojJCRk9pVKJ2NhYHDp0CHFxcUhPT0dBQQHS09MRFxeHQ4cOISYmBkql0tChEhER2M8J3SMiIiKQkJCA9evXY9q0adJwZ2dnJCQksJ8TIiIZYXJC94yIiAj07t0barUaOTk5cHBwQFBQEGtMiIhkhskJ3VOUSiVUKpWhwyAiolqwzQkRERHJCpMTIiIikhUmJ0RERCQrTE6IiIhIVpicEBERkawwOSEiIiJZYXJCREREssJ+TojuYdnZ2cjLy2vQtJmZmXr/NpSdnR0cHR0bNQ8iMi5MTojuUdnZ2Rg/YTxKiksaNZ8lS5Y0anpzC3N8sOUDJihEJGFyQnSPysvLQ0lxCUz6dIaJvbVBYhC5BSj56TTy8vKYnBCRhMkJ0T3OxN4aJg4tDbZ8YbAlE5FcsUEsERERyQqTEyIiIpIVJidEREQkK2xzQveUkpIS7Nq1CxcvXoSrqyuGDRsGc3NzQ4dFREQVMDmhe8aGDRvwySefQKPR6A0bMWIEYmJiDBgZERFVxOSE7gkbNmzARx99hFatWuHpp59GWFgYDh06hHfeeQcfffQRADBBISKSCSYnZPRKSkrwySefoFWrVti5cydMTct3+8GDB+Ohhx7C448/jk8++QRPPfXUPXmLR+QW3JPLJiL5YnJCRm/Xrl3QaDR4+umnpcREx9TUFBMnTsSqVauwa9cujBgxwkBRGo746TT7GiEiWWFyQkbv4sWLAICwsLBqG8SGhYXplbvXGLqHWPHTaYMsm4jki8kJGT1XV1cAwLJly/D7779XaRAbEhKiV+5ewx5iiUhu2M8JGb1hw4bBxMQER48ehY2NDe6//348/PDDuP/++2FjY4OjR4/CxMQEw4YNM3SoREQE1pzQPSYvLw8//vijocMgIqJasOaEjN6uXbsgRO03D4QQ2LVr112KiIiIasOaEzJ658+fBwDY2dlh+/btSE5OlhrEDho0CGPGjEFeXp5UjoyHRqPBn3/+CQD4888/4e3tDaVSaeCoiOh2mJyQ0cvJyQEA9OzZE9bW1lUeF+7evTv27t0rlbvXGGs/JykpKUhMTMSVK1cAAKtWrUJSUhKeeeYZRERE3LHlElHjMTkho1JUVITMzEy9YQpF+d3LgwcP4uTJk3p9nZSVleGXX36Ryp05c0Ya5+HhAUtLy7sQtWHY2dnB3MIcJQbu58Tcwhx2dnYNmra67Q0Ax44dw8aNG6sMv3LlCuLj4zFlyhSEhobqjTP27U3UnJiI292Ml5n8/HzY2dkhLy8Ptra2hg6HZObMmTOYMmVKk8xr48aN6NixY5PMS66ys7ORl5fXoGkzMzOxZMkSzJs3Dx4eHg2Owc7ODo6Ojg2altubqPmoz/n7nq850Wg0UKvVyMnJgYODA4KCgnhPuhnz8PCocsVcVlaG6dOnw9TUFKWlpXqNY01MTGBmZoaysjK8+eaberUqjTnhNheOjo4NTgx0PDw8DHZSr257p6enY+3atbC2tsaKFSuq1JTNnj0bBQUFmDFjBgICAvTmRUTycE8nJykpKVi3bh2ys7OlYY6Ojpg6dSrvSTdTlpaW1Z4oR44cKb34z9PTE8eOHUNoaCgyMjJw/fp1jBo1Cv7+/gaImBqjuu398ccfAwCio6Or3aZPPfUU3nrrLZw8eRKPPvroXYmTiOrnnn2UOCUlBfHx8cjNzdUbnpubi/j4eKSkpBgmMLojYmJiMGrUKOTn5+PYsWMAytsl5OfnY9SoUXwjsREpKioCADg5OVU7XjdcV46I5OeeTE40Gg1Wr14NACguLtYbp/u8evVqvW7OqfmLiYlBcnIyRo4cCaC8NiU5OZmJiZEJDAwEALzzzjsoLS1FWloa9u3bh7S0NJSWluLdd9/VK0dE8nNP3tY5fvx4lRqTynJzc3H8+PEqLfqpeTM3N0f//v3x8ccfo3///jA3Nzd0SNTEHn30UWzatAnnzp3D0KFD9S5ALCwsUFxcDIVCwVs6dMfU9BRZfRnqCbIHHnigyrAffvjhrsZwTyYnR48erXM5JidEzYu5uTnCwsKQmppaY81oWFgYE1O6YzIzM5vkKTJDPEFWXWKiG343E5R7Mjn59NNPqwxr06YNrl27VqXc5MmT71ZYRNQENBoN0tPTay1z8uRJaDQaPplHDXK7R/BLSkowb968GsdfvnwZ7777Lp566ik4OzvXOp+KfS9V1pjH8KtTU2JScfzdSlDuyeSkpKRE+v/kyZOxefNmXLt2DQqFAtHR0di0aVOVciQfjembA4BU3dqYatemPihQ00lLS0Nubi6CgoLw2muv4auvvpJeVzBkyBC8+OKLUKvVSEtLQ9euXQ0dLjUz2dnZGD9uPEpKG39+0LV/aihzM3N8sPWDJjkWVU5MKiYhFcfdrQTF6JOT29370yUiAKDVavU+A7inegxtDrKzszF+wniUFDf+wLBkyZIGT2tuYY4PtjTNQYGa1vHjxwEATz75JCwtLau8rmDChAmYNWsWjh8/zuSkjubPn4/U1FTpc+/evbF48WIDRmRYcnlY4k7F8cYbb+glJG+88QaeffbZO7KsmhhFcnL69GlcuHCh2nG66rOGqnjfsLYqODc3N3Tu3LnBy2nu7lZndnl5eSgpLoGqB9DSQB0E38gH0n4tQV5eHpMTGdJ1smdiYlKnclS76qr6U1NT73obBLlwdHTEW4lv1XjOARp/3tG53W0fNze3Bh+Dartwr5yIVP5c+VbTnbhwb/bJSXZ2NqY98ww0Wu0dX1ZtO5tSocC27dvvyMlKDi2na6PrM6ayhQsX3rHO7FraAnat7sisDU7u21vuQkJCkJSUhPfeew/BwcHSu5WA8trR999/XyonB3l5eZg/fz6ys7Ph6OiIxYsXN/hdQ01NTm0Q6mLw4MEoKPi/l0laW1vj66+/bvLldO7cudaL0aKiIvTs2bPRy2nsSf9OXbhXbux7Jy7cDZacvP3221i7di0UCgVcXFywefNmuLq6NmheSqXyriQnt4vhTpBLy+ma1JSYAEB8fPwdS1Bu5jf5LGWxbLlv7+YgODgY9vb2UKvVmD9/PqKiouDp6YmMjAxs27YNJ06cgL29PYKDgw0dKsaOHYuLFy9Kn69evYrhw4fD1dUVSUlJBoys/FaOzogRI/DMM89InxMTE/HJJ59I5eRwi6e6305BQYFBfjs19VR9N5VfuE+DRnvnb0HVfuGuxLbt2+p94W6Q5OS7777Dpk2b8PPPP8POzg47d+7EY489hsOHD9d7Xo6Ojvhg69YaG0jqXk7WFGp7wdmdaCAp96sWjUZTY2KiEx8fj7179zZ58vb7r006O1mQ+/ZuLpRKJZ577jnEx8fj2LFjOHTokDTOwsICAPDcc88Z/EmdiolJjx49MH78eHzwwQf49ddfcfHiRYwdO7bJE5S//voL//zzT7XjCgoKcO7cOelzxTYmJSUlUseVlaWmplYZ5+3tDWtr62rLd+jQAT4+PvWMvHb87VSv/MLdsO1jGvo7M8hbiR977DFMmjQJAwcOlIaFh4dj3bp1UKlUtU5b37cSV3dfrT7Pn1d8qdjdbBBb15bTlcfdTYaIUfcW2o4BQIsWTTLLert1CziT3rR9EMhxe9+uMXl93kpsiMbk1b07y8nJCbGxsQZ/d1ZeXh6GDx8OAPj666/1TuQFBQUYPHgwAOCLL75o0ls8zz77rNRg2FCCg4PxxhtvNNn8Kt7K8fb2xubNm6Vx0dHRUsJ1p27xyFltTzaWlJQgKytL+lyfi/jKj0k7OTnV2G9QxQv3+py/DZKc2NnZ4dKlS2hR4ewyd+5ctG7dGrNnz9YrW1xcrNeRUn5+Ptzd3eucnFTnq6++wuuvvw4AGDVqFD766CNpXMXPL7zwAoYMGdKgZVRW2xULUPWqZdeuXdL/hw0bVqV8TeNru2IBbn/VUp8rq4bGeLs4a4uxKZ/WaYzbPa3TXLZ3bXSJYFMwRGdSwN1rqF3f7Z2SkoLc3Fy0bdsWYWFhVcofPHgQ165dg729vV4iZey/7/rG2Jg473btjtzdrvapooZeIMk6Obl58ybat2+PnJwcveHr1q2DWq3G+vXr9YYnJCRg4cKFVebTmOREo9Ggf//+0mczMzOMHj0aO3bsQGlpqTS8KW9HyOGKBbj9VYsc4rxdjLVdDcjlNp4c1iPQuKvUpuqCGzD+x/CNYXtXVvHx4dranDT1Y8VyWJdNXbvTHBw/flx6Kmf9+vV6jVhPnz6N2NhYAOWPFTe0vZask5MLFy6gZ8+eeo3AgPIGNQcOHMCWLVv0ht+JmhOg9oacQNM/adJcrqQbemU1dOhQvUc3hRDYvXt3jd/hTl21yOWE2ly2NzWN5lJzUl91uZpu6tuMrDkxnMrbOyIiAikpKXrDGrO9ZZ2cXL16FZ07d65Sc5KYmAi1Wo0NGzbUOn1925zUJiUlBatWrdK7Crezs8Pzzz9v8HvScmyDUFnlON566y3pqYhp06bpjbsXG6PVR3PY3tR0DNXmpCFqS1DksC+yzUnTupPbW9bJiRACLVq0wJUrV2BjYyMNnzNnDmxsbPQeX6tOUyYnwN27J90Qhrhqqa/mEGNzwXV5b6n4tE63bt0wbtw4bN26VXoxqRweJ9aRew+x/O00rYq3eIDG3cqpSNbJCVC+I82ePRuDBg2ShoWFhWHJkiXo27dvrdM2dXIid3K/agGaR4zNBdflvaVyPyc6ckpMmgv+duSvPudvRa1j75AZM2ZgwYIFyM8v781q586duHXrFu6//35DhCNrNf2o5PRjaw4xNhdcl/eWpKQkfPHFFwgMDETbtm0RGBiIL774golJA/zwww9V2pBYW1vzt9NMGaTmBADWrl2LjRs3QqFQwMnJCZs2bYKnp+dtp7vXak6IiIiMgexv6zQGkxMiIqLmR/a3dYiIiIhqwuSEiIiIZIXJCREREckKkxMiIiKSFSYnREREJCtMToiIiEhWmJwQERGRrDA5ISIiIlkxNXQA9aXrM07X9T0RERHJn+68XZe+X5tdcnLjxg0AgLu7u4EjISIiovq6ceMG7Ozsai3T7Lqv12q1uHTpElq2bAkTE5MmmWd+fj7c3d1x/vx52XaJzxibTnOIkzE2neYQJ2NsOs0hzns1RiEEbty4ARcXFygUtbcqaXY1JwqFAm5ubndk3ra2trLdUXQYY9NpDnEyxqbTHOJkjE2nOcR5L8Z4uxoTHTaIJSIiIllhckJERESywuQEgIWFBeLj42FhYWHoUGrEGJtOc4iTMTad5hAnY2w6zSFOxnh7za5BLBERERk31pwQERGRrDA5ISIiIllhckJERESyYpDkZOPGjfDz88PEiRMBADNnzsSRI0fuyLKWLFmCbdu2Nek8CwsL4e/vj/nz59dpub///jueeeYZvfHfffcdXnnlFemztbU1NBpNk8ZZk8LCQgwfPhyxsbG1rvfU1FRMmjRJmiY8PLzW79wULly4gNGjR0MIIcV58+ZNvTKTJk1CamoqgMZv3+3bt+Opp55qVMxAzdtPt6936tQJH3zwQaOWsXTpUnz11Vc1jr9w4QIefPDBBs374MGD6Nu3r96wBQsWwN/fHy+//HKD5gnUbf1u27YNS5YsAaC//WvTsWNH/Pvvv9Lnw4cPIygoCKGhoSguLq7zvCruS3Xl7+8v/b/isash86rodsfBefPmoU+fPg2ef2OlpqZW2Ud0mvr4tXTpUr3j4+08+OCDOHDgQJMtvy4uXrwIb2/vJpnX0qVLsXDhwkbNY/LkyY0+xtxO5ePEmDFj9H6HTckgnbCtWLECv/32m9QZy5o1a+7YskpLS1FaWtqk87SyssLjjz+OsrKyOi03JCQE/v7+yM/Ph62tLS5evIjs7GwsWLBAKl9YWFin9w00BSsrK3zxxRd6w5YvX46XXnpJr9fdit/BysoKBw8evO00jeXm5oYdO3bUGGfluBq7fUtKSlBSUtLg6XVq2n66fX3mzJmNXs7tkoSysrI6L+PmzZt47733MH36dABV10NBQQE+/PBDnDx5EmZmZtLw1NRUaDQaRERE1Gk5dVm/Fbdhxe1/u/lW3O5vvPEG3njjDfTr108aVpd5NWT/KSgokP5f8djV2H3xdsfBAQMG4OLFi9Ln+m6LxiotLa1xWzb18aukpKTW42t15Ruz7iv/HuqitLRUSoQbq77ft6Z5NMWxrD7L2L59u974xMREjBs3rkk6bTNIzYlWq61zL3HGYsWKFbhy5QoA4OzZs9i0aZOBI9I3d+7cel/5NGSae41c9/Vr167h1VdfrXF8Tk4OXFxc9BITANizZw++//77Ox1evV25cgVeXl6GDuOukuu2aI5u93uguql4nmusu5qcbNy4ESqVChcuXIBKpcKqVasAlFfJpaSkAACGDx+OdevWSdPEx8dj3rx50ud9+/YhJCQEfn5+CA0Nxd69e6VxpaWlmDVrFgIDA+Hv74+oqCjk5uZWG8vMmTOxatUqPPbYY/Dz80OXLl3g5uYm3bbIzs7GmDFj0LZtW5ibm6Nly5bw9fXF0aNHAZTfTtBqtdJyu3fvDktLS1haWqJVq1ZSuX379sHX1xeZmZkYNmwYfHx8EB0djaNHj6J169b44IMPEBcXpxebVqtFu3bt4Onpic6dO2PkyJHIycmpcb3Onj0b/v7+CA4ORnBwMD777DNp3Pvvv48OHTrA2toaFhYW8PHxQWJiIlq0aIEBAwZg3rx5UKlUAAAvLy+0adMGXbp0gaenJ9544w2kpKRApVLh0KFDsLCwwMSJE7F161ZpGhcXF9jZ2cHKygqWlpaIjIxESEgISkpK8Mwzz2DYsGHw9vZGy5YtYWtrC39/f/Tp0wdpaWkYN24cAgMD0bVrV/Tr1w9+fn7w9fWFubm5tP6sra0xceJEaZuGhITgiy++wIQJExAVFYVffvkF77zzjt762Lp1KyZPnix9fv311xEYGIguXbogICAA69evr3FdVnT9+nUMGTIEwcHB6Natm3TrYd68eWjRogX8/f0REBAgxerr6ws/Pz8EBgZK3zkjIwOWlpb45ptv8O6776JHjx7YtWsXVCoVTE1NYWZmBjMzM9jY2MDDw0Pa3q1atcK8efNgb28PS0tLDBgwAPb29nBxccHq1asBAN988w1CQ0MRHByMHj16ICUlBf/99x9CQ0MREhIClUqFBx98EL6+vrC3t8eQIUOg1Wrx6quvSlfh5ubmcHNzwzfffAMAWL16Ndq0aQMfHx/89NNPaN++vV5N1rp167B582aoVCqcPHkS3t7eWLBgAQIDA+Hk5AQnJyd0795d2mcqE0Lg1VdfRVBQEPz8/ODo6Ig5c+Zg06ZNWLJkCS5evAhXV1ep6risrAzz5s2Di4sLWrRoge7du2PPnj3S/J544gm0a9cOhw8fxuDBg9GrVy9ERUVh6dKlmDVrll61e1xcHLp27YrAwEB07NgR7dq1w+nTpzF58mTMmTMHALBjxw4EBATA398fvXv3RlJSEsLCwhAcHAyVSoVPP/0UQPmV9lNPPQUbGxt4enpi2rRpKCkpwZEjR9CtWzc4OzvD3Nwcbdu2RXh4OH7//Xf89ttv8PHxwX333YdNmzYhMDAQFhYWCAoKwoEDB6Tj4LFjx9CrVy94eHjA2toanp6eiIyMxOuvv47jx48DAHr37o3ExESsWLEClpaW8PLygre3t96tjQsXLsDPz69O+7pO165d8fnnn6NXr17SbbLKNab1tXLlSgQEBCA0NBTh4eEQQqBTp07IzMzUK+ft7a1XM6Rz8OBBTJkyBTNmzEDXrl3h7e2NsWPHSi+B1fnss8/QpUsXBAYGIjg4GPv27ZPG5ebm6h3r+/Tpg/T0dADAq6++ikGDBuHKlStQqVRYvnw5gPLjsG7fs7Kygp2dHfz8/BATE4OCggI899xzyM7ORteuXaXb9u+//z78/PyqHBt05xNPT0/4+Phg6NChOHfunF786enpiIiIQEBAAPz8/PD666/rjf/rr78wZMgQeHl5wcvLC1FRUVUSAY1Gg+nTp8Pb2xuhoaGYMGEC4uPjpRqO6m7f/vTTT3q3g5OSkqBSqRAUFAR/f3+9Gv7KdNtx3759UKlUuHTpEoYNG4YRI0bg3Llz8PHx0atRW7lyZd1v1wkDaN++vd7nyMhIsWfPHiGEEJmZmcLNzU1cvnxZpKeni44dO4pbt24JIYQ4f/686Ny5szh37pwQQojTp08LDw8Pce3aNSGEEMuXLxcPPvigKCgoEEII8dFHHwkzMzPx3nvvVYlh3759IjIyUvzyyy/SsHbt2olu3boJIYTo0aOHGDhwoBgwYIC4du2a2LZtm/Dy8hJFRUVCCCEAiLlz50rLDQkJETdu3BBCCLFmzRoBQCQmJgohhPjhhx+EhYWFOHv2rPTZx8dHREVFCSGEOHHihAAgSktLhRBCzJw5U7Rr1076vGLFCvHEE0/UuD6Tk5NFWVmZEEKIs2fPCgcHB5GbmytWr14tevXqJby8vMQff/yhNw0AERERIa13AMLFxUX8+++/Qgghpk6dKpRKpRgyZIjeNBXjACBmz54tSktLRUREhDA3NxcnTpwQQghRXFwsbG1txdixY8Xly5fFL7/8In2PpKQk4ePjI6ZNmyaEEGL16tXSes7IyBCurq56yxg/frwoKysTy5cvF76+vsLU1FT88MMP4qOPPhJKpVL06tVL77u9/fbbYsKECdLnPXv2SNvt6tWrwsXFRfz5559CCCHee+89aTtUtmDBArFy5Urps1arFatXrxbdunUTPXr0qLI+XVxcpG3m6+srfH19pX1dpVIJExMTERwcLHr37i0cHBzEG2+8IYQQ4rXXXhPOzs7CwcFBCFG+vc3MzIS9vb04d+6cCA0NFd9++62YMGGCePPNN0WvXr1EUlKSaN26tUhLSxNCCHHx4kXh5+cnrKysxIULF4QQQgwZMkS8/fbbQgghxo0bJ1Qqldi4caMQQojHHntMmJubS7+N/fv3Cy8vL9GrVy9x5coVkZGRIXr27Ck6d+4svv32WyFE+e92+vTpYt68edL3dnV1FVu3bhVFRUUiPj5etG/fXrRt27bG9btjxw4RHBwscnJyxIIFC8TUqVOFjY2NiI+PF1qtVmRkZIg2bdqI3r17CyGEmD17thg2bJjYvXu3iIyMFL///rtwc3MTLi4u4uzZsyI/P194enqKbt26iSNHjghPT09x6dIlER8fL5555hm9fSk0NFQEBweLsrIykZGRIVq0aCFMTEzEDz/8IIQQ4siRI6Jr167iypUrQojyY4RSqRRff/21EEKIvLw80adPH9G+fXsRExMjnnzySek39NprrwlTU1Ph4OAgPvjgA9G1a1fxzz//iEcffVSMGzdO+Pj4iJKSEjFz5kzh7u4ugoODxWOPPSbWrVsnDh48KJydnUWfPn3Enj17REREhHj//feFq6ur+Pfff4VWqxXp6enCxsZGBAUFSfuik5OTGD16tBBCiPz8fNGhQwcxcOBA6fu+9tpr4uWXX652366Jr6+v6Nevn3Qs2717t3BychKFhYXihx9+kLZLZRWPXxXp9t/i4mIpbiHK9yXd8VDH1dVVZGRkCCGEiI+Pl/azH374QbRo0UL6vZSVlYkxY8ZIxw8hys8hYWFhIicnRwghRGpqqnBxcRElJSVCiPLffcVjfVJSkt53qXzcEUKIlStXiokTJ4rU1FRx5coVsWLFCjFq1Cjx8MMPiyeffFKMHz9emkZ3bHjwwQel81FFPXr0EIsWLZK+f+XzSXx8vGjdurV0/Lx27ZpwcXGRjtuFhYWiffv2YsuWLdI8ly9fLsLDw6XPEyZMECNGjBB9+/aVzoEff/yxaNmypfQ7r24b7tmzR0RGRkqfDxw4IPLz84UQQty8eVMEBwdL54nK01fejpU/d+vWTaSmpkqfAwICxJkzZ6qsn+rI7mkdd3d3zJkzBy+88AJiY2OxZs0aWFtbAwDWr1+PadOmSdW3nTp1wsMPPyw1EkxKSsLChQthZWUFAHj88cfRq1evapcTGRmJM2fOoHPnzgCAtLQ0eHt7Iz09Hfv370dhYSGOHDmCLVu2wMHBAWPGjEFgYCA+/PDDKvNKSkrCunXrYGNjAwCYMWMGbGxskJWVVafvHBAQIMUAAO+88w5eeuklmJqWNwl67rnn8PXXX9d4C2XgwIFQKpUAAB8fH3h6euL48eNYsmQJzMzMsGLFCgQFBd02jqlTp8LDwwMA0LZtW3To0AEODg7Vlq3YINXU1BQmJiZ6V7XJyckoKyvDW2+9BScnJ/Ts2VP6HkOHDsW///6LsrIyFBYWYsmSJdJ6rs6iRYugVCqRlJSE999/H66urgDKt29dGqT1799f6uWwTZs2CA8Pl9Z1bbRard594KKiIixZsgRz5sypcrsDANq3bw9TU1Ncv34dFy9e1Ku169KlC+zt7QGU97wYFRWFmTNnAgDefPNNHDx4EDk5OcjIyMBzzz2H0tJS9OzZE+fOnYOXlxceeughAIClpSVmzJiBN954AyNHjkRwcDCA8hqsWbNmASivyTt79iyysrIQHR0NoPyFmQ8//LC0/+pq/XRMTExw4cIFrFu3Dm3btpWWtXTpUixbtqzW9XTfffdJ69fPzw+RkZE1rt+kpCTMmTMHrVu3hlarhYeHBx599FEphopu3bqFTZs2YdOmTbC0tAQAqFQqPP/888jLywMAtGzZEm+++SbOnDmDxYsX48UXX4Szs3O1yz5x4gT69esn/VZu3bol7UtAea3RwoULpe9//fp12Nvbo0WLFgDKX4CWkJAA4P8a8epifuGFF2BhYYFRo0bh22+/xcKFC9G+fXusX78eu3fvRvv27XHw4EEsWbIE165dg7+/Py5fvoyYmBiEhYXB1tZWasui1Wrx1VdfYdq0afDw8ICJiQn8/f0RGhoqxbp37160atVK2v9btmyJOXPm4Mcff5T22W3btmHs2LG1brvKSkpKsHDhQulYNmTIEAQFBeHrr78GUH6MUqlUVf5qIoSAEELa3xraPs3FxUX6vSiVSqxYsaJK489nn30WrVu3BgCEh4fD2toaf/31F4Dy333Pnj2lssOHD8exY8dqXeaaNWuwevVqhIeHo23btnjuuefwzTffYNCgQfjnn3+qPTa8//77VY5j+/fvR1FREebPny99/+rOJ6NGjZLOBQ4ODhg0aBB+/vlnAOVtO4KDgzF+/Hip/EsvvYRbt27hxx9/lIYdPHgQixcvls6BI0aMQI8ePWr9npVFRESgZcuWACDVsN9uXdVk7Nix2LlzJ4DyfadFixbw9fWt07SyfCvx1KlT8c4778Dd3R0PP/ywNPzkyZPYsWMH3n77bWnYzZs3ERgYCAD4999/pY2r07Vr12qXoVQqMXDgQDz//PM4f/48jh07BnNzcxQWFkKtVqNTp07QarV6B7r77rsPf/zxR5V5/fvvv7h8+TL+97//4cyZMxBCoLi4uN6Nkz7++GN4e3tLjbMq/vhsbGyQk5ODdu3aVZnuu+++w6ZNm6Rl//PPPzh58iQcHR3x22+/1bnBXOV1V/lzRbqq3ooHm8ceewwfffQRnn32WWzZsgXFxcWIjIwEUN6GITc3F0VFRejVqxdKS0tx69Yt9OjRA9bW1tWeUP777z8A5QcmoHw9BwUFISQkRCrTvn37KtW7lf36669Ys2YN0tPTUVpaisuXL2Pw4MG1TgOUH+zGjRuHffv2ISEhAWZmZnB0dKwxidIlLOfPn0f79u1x+vRpKdkzMTGR3qZ969YthIeHY/369di5cyfOnz8vVb/369cPtra2MDExQefOnXHy5Enp1pqu+rRly5YoKyurdl/38fHBww8/DD8/P5w5c0Y6cWRmZsLKygqenp4AgOeffx67du3CunXr0L17d9y8eRNlZWV66xYo3+effvrpWtdTWloa5s2bh3379qG0tBRCiBrXb8XfqG79/v3339Ume3/99RdcXV3h6OgItVqtF1PFxo+DBw+GmZkZzpw5g88//7za5f73339QKBTSQRcAWrVqhW7dukmfT548iVmzZkm3ka9evYrS0lJcv35dKtO1a1doNBqYmZlJ+yVQnvyZmpoiKCgIGzdu1JtPYWEhTp06hevXr6NFixbw8vLChx9+iGPHjkm/nzZt2khPpa1ZswYPPPAA8vPz8cQTT6B9+/YAAGdnZymWkydP4t9//8Xbb78tXZyVlJSgVatW2LdvH9zd3WFqalrv2zoAqiQbQUFByMjIgIODA1QqlXTCrKimpMPb2xtjxoyBSqXCjBkzEB0dDXNz83rHpEvCdVxcXGBqaopr166hTZs2ACAlJjrt2rXD1atX4efnB61Wi40bN+Lzzz/H+fPnYWZmhsLCwhqXl5eXh8uXLyMyMhKlpaW4cuUKbt26hdLSUixfvhx9+/bFrVu3cOXKFSQnJ6Nt27ZwdHSs9jimVqtx3333VRle+XxSU/y1zaN37974448/cP/99wMobzujOx/qVP5N386ff/6JFStW4NixYygsLMT169cRExNTr3nojB49Gj169MDq1auxfft2jBs3rs7Tyq7mBCg/gOXm5kKtViM/P18aXlhYiGXLliEtLU36++uvv6QW1gqFokqL8cpXiBUVFBRg9+7dWLp0Kdzd3aX7tborq8qtp4UQ0rjKy5gxYwaefvppHD58GCdOnECrVq3q/b0/++wz7N27F0qlEmq1Wu97Xrx4sdrEZO/evZg0aZLesnUHJF38dW0FXnnd1fcg0qVLF+Tm5uLkyZM4cuQILCwskJaWhqFDh8LLyws//PADioqKpMclP/jgA8yZMwdXrlyptpW8QqGo8rniVVh1MQP6T1OcPHkSQ4YMwdChQ5GSkoL09HS9Jzpq4+DggOTkZMTFxSE6OhpJSUl1Wpe3bt2Cubl5ldh0NWFmZmbYuXMnduzYgZdeegnW1ta4du0agPJalLS0NNja2sLFxQWFhYX43//+h7S0NAwbNgzx8fE4efIkBg0aVO2+3rp1a6SlpcHGxgbFxcX46KOPpGkXLlwonVjc3d1hbm6OsLAwDBw4sMZHYGva53Vu3LiB6OhoDB06FBMnTkRsbGyt67fib1S3fgcMGIBdu3ZVucdecbkVt2nl752Xl4dbt24hPz+/xscaqzs2WFtb6+1LhYWF2LJli/Sbmz17Np5++mkMHz5cKqPVamFiYlLrkymV5+Pl5YXvv/9ems/169fRrl07vbYzFYWGhuL++++Hn58fevXqJW2zisvUdWcwadIkaTknT57E66+/jp07d+LDDz+sd62JTuWnXgoKCqQr8YaYNWsWDhw4gGPHjqFPnz41JgUVt/HtYgLK10FtcZmYmEjbNz4+Hjt27MDSpUtx4sSJattDVZ63ubk5jhw5gqKiIsyYMQNnz55FaWkpXn75ZSiVSqxatQqtWrXCkiVLsGLFihqPDTX9fm7326oYf13nYWZmVuUJottdJFdc79euXUNERAS6du2K77//HqdOnapXQlGZo6Mj/Pz88NNPP+Gzzz7D6NGj6zytLJOT2NhYLF26FFFRUXqNYX19fWvtB6Bjx444ceKE3rDqsnyd48ePw8zMDHZ2drCxsZGuSkJDQ3H69GlcuXIF//zzj1Q+NTW12iy0RYsWGD58OAYNGgRra2uUlJRUacBa8cqiph3Nzc0Nr732GszNzaXGWrfzxRdfYObMmXrL/uuvv9ChQwepgVdNB8HK8VVed+fPn6+xfFhYGABUudU0ZswYPPnkk3jkkUdgamqK9PR0fP7551i1ahW6desGpVKp990effRRWFpaYsuWLdJVgo7uNoiukVzHjh2RlpaGw4cPS2X+/vvvKv2g/P7779L/k5OTMWrUKIwePVp6vO3kyZO1rovK+vTpg++//x6bN2/GlStXUFBQICUT1enYsSP+/vtvvWFarVY6cfr6+uLHH3/EqlWr8OCDD8LU1FRqxFz5CrGmfb62fd3S0hLPPvsszMzMpMdT7e3t9WJWKBQoLS1FSEgItm/fjh07dsDc3LxK9W3Ffd7e3h5FRUXSuH///Rf5+fkYPHgwRo8eLd3aqW39Vhf3pUuXMG7cOMTHxwMo3xevXbsGX19fZGVlISsrS2+bpqam6iXOc+bMgbOzM6ZPny4lufb29no1Hvb29tBqtXoXOwD09qXK67qmdaxQKGBubq7XeFP3mO0ff/yhN5/s7GxcuXIFPj4+AIBDhw4hLy8Pa9euxZtvvlmlUahOp06d0K5dO6xYsUJqoFmxrK+vLy5dulRlumHDhmHfvn34+OOP8cQTT1Q779upfEvut99+0+vbpSEcHR2xefNmtGzZEl9//XWV/fHff//V216V/fHHH3rJ2cmTJ9G2bVvpltvt1HYMAqoek9u1ayf9Lq2trREXFyfdAqw4rZmZGfbs2YNvv/0W2dnZeucLndDQ0GrPQzWdT6oTGhqKn376qcrwgwcP6s3Dycmpym+44vasvN4B/ePljz/+iB49eiA2NlaqkarruQio/twWFRWFWbNmoXPnztI860J2ycmHH34IrVaL0aNHY+7cudi9ezd+++03AMCTTz6JzZs3623oijtDbGwsEhISpMw8MTERGRkZNS7L1dUVHh4eeP755zF06FAkJCSgRYsWCA8Ph62tLUJDQzF+/Hjk5ORg69atOHXqFEaOHFllPr1798ann36KgoICCCEwaNCgKjU2pqam0snJwcEB169fr3L1FRUVhb/++gtPP/00pk+fLrVZKCsrq7YVO1Be1av74QohMG/ePCiVSlhaWmL27Nm4desW5syZI7Xyr0mrVq2wYcMG6QB44cIFnDp1qsby9913H8zMzBATE6OXoERFReHIkSMYP348Jk6ciOnTp6NNmzY4fvw4ysrKcPr0aSQkJMDa2hpCCFhbW2P8+PEoLCyssU1NXFwcNBoNYmJiMH78eOlKIDExETk5OTh37py0nX/55Re9H7GzszPS09Olq5q33noL2dnZta4LnYoHS7VaDTc3N8yePRvLli1Ddna2dPV19uxZvekcHBwQFRUlfRZC4Pfff0deXh7MzMwwePBg3Lx5E0lJSVAoFHjiiScwadIkKJVKuLm5oaysTNp/Bg4ciEuXLmHt2rXS/LKzs/Hoo48iOTlZ2q5//fUXNmzYIH3P4OBgmJqaQq1WQ6vVomfPnvjwww+lE9q7774LrVaLoqIiqNVqtGnTBq6uroiNjZWeACgsLMTcuXPx0ksvAQB69uyJU6dOSe2F5syZg5YtW+Ls2bPSco8cOVLr+o2NjcWrr76K//77D9evX8euXbvw008/ITs7Wzr4m5qa4urVqzh27BhiYmLwxBNPSMnbsWPHsGrVKinRPHjwIH7++We4u7tj5MiRuHz5Mj7//HP07NkT+/fvl/apL774ApaWltIwIQTy8vL0riqffvppLFu2TEquHnroIZw9e1Z6XPfq1at47bXXAABTpkzBvHnz9H53uhqxyMhILFu2DL/99humTJmCqVOn4vLlyygtLcWUKVPg6+sLBwcHzJ49u9oaw+vXr2PSpEnYtGkTUlNT4erqisOHD+slfQMHDsStW7fw3Xff6e0XJiYmiIyMhKenJxwdHWvcDrV55ZVXpCRu69atKCwslG4b1FdBQYG0jvPy8nD+/Hm4urqiZ8+e2LJlCwBI+5KurU91Ll26JD3dWVxcjNmzZ2PatGl1jsPZ2Vn6reTn50vHep1WrVohNzdXutBRKBSYOHEi1qxZIyWYZWVl2LVrF7766iuUlJRIx+9z585BqVTihRdewIQJE6pcmIaHh8PGxgavvPKKNE1t55PqjBgxAunp6XjvvfcAlB9Tli5dCjs7O4SHh0vl+vXrh7i4OGn7ffLJJ/j111+l8R07dsTVq1f1jl0V+5JydnbG2bNnpdqUL7/8sl4dpDo4OFSpvXz00UeRnp5e/xqYOjWbbWL+/v56nwcMGCAOHDggbty4Iby9vfVa+3700Ud6T0bs2bNH9OjRQ/j7+wuVSqX3JIBWqxXz588X3t7eIjg4WEyePFnEx8eLpKSkauM4c+aMCAgIECYmJiIwMFB8+eWXwtvbW5SUlIhr166J8ePHCwcHB2FmZiZsbW2Fj4+POHz4sBBCCDMzMxEfHy+EKG8pHxgYKMzNzYWlpaXo1q2bUKlUYs6cOUKI8pbj/v7+okOHDiI8PFycOXNG9OjRQ9jZ2Yknn3xSCCGElZWV2LVrl4iOjhYlJSVi/vz5ws/PT3Tp0kWEhISIDz/8sNrvcOPGDTF27FgRGBgoAgMDxeLFi8XEiRPF3r17hRDlTw65u7sLS0tLYWFhIXx8fMSaNWuEtbW16N+/vzhw4IAQorxlv6Ojo7CyshKBgYHC19dXPP744yI6Olpalrm5uRSvEEIsXrxY2NraCktLS2FjYyM8PT3FtWvXpO2r+x7e3t6iRYsWwsrKSnTo0EF8+eWXonXr1sLDw0MEBgaK0NBQERsbKzp16iT8/PyEmZmZtJ6trKzE5MmTpW3au3dv0bJlS+Hj4yNt30mTJgk/Pz8RGhoqhg4dKrZu3SomTZokhBCitLRUzJgxQwQEBIiAgAAxc+ZMMX/+fLF582YhRHmr+YkTJ1a7bqdOnSo6dOggAgMDRVhYmDh69Ki0Tt3c3ISlpaWwtLQUXbt2FRYWFqJ///7StLdu3RKmpqbCzMxM+Pj4iKCgINGhQwfRv39/kZKSIr788kthb28vzM3NhZmZmfD19RVmZmYiKChIhISECBsbG/Hmm28KIcqfeBg8eLCws7MT7du3F3369BHZ2dnixx9/FMHBwaJLly6iT58+4tNPPxVWVlaiU6dOolOnTuKJJ54Qo0aNEp06dRLBwcHCyclJeHp6CpVKJby8vIS5ublQKpWidevWYufOneKBBx4QiYmJonPnzsLHx0fY2NiITz75RPpOWVlZom/fvsLS0lJYWVmJqKgo0bNnT/HUU0+JgIAA0a5dO9GrV6/brt+33npL+Pr6ijZt2ghra2vh4uIi3N3dxdGjR8WFCxeEl5eX+Oqrr6R9o0OHDsLZ2VlYW1uLnj17ih9//FH4+vqKjIwMoVKpxE8//SQGDBggLly4IFJTU4Wnp6coKioSL774ojAzMxMhISFi/Pjx4tVXXxXdu3cX3t7ewt/fX7Rq1UpER0eLn3/+WYpt+/btIjg4WAQEBAiVSiWefPJJ0atXLxEYGCgt29/fXxQXF4spU6YIKysr4ePjI+bNmyeio6PF2rVrRY8ePYSTk5MwNzcX7dq1EyqVSsyePVu89tprYsqUKdLxrrS0VHTp0kV8/fXXom/fvqJ79+7iwIEDQqVSCV9fX+Hq6ipatmwpAgICxEMPPSRGjx4tQkNDpVgPHTokbG1thYWFhXB1dZX2i9mzZ9d4zLud9u3bi88++0x06dJFeHt7i/vuu09kZmYKIcqPYw888EC101lZWUlPDFa0f/9+4eHhIfz9/UXnzp3F66+/Lu1LgwYNkvb3LVu2iF69eklPmi1ZskQsXLhQCFH+hMiYMWNEbGys8PPzE66uruLFF1+UnnwR4v/OIRX17dtXelLkzJkzIiIiQgQHB4vQ0FC9Y73OzJkzhY+Pj3jooYeEEP93/HJ2dtb7re/YsUOoVCrh4uIizMzMRGhoqPjuu++EEOXHhk6dOokuXboIf39/6TimO594enoKb29v8eijj4p//vlHWnbF76vzyiuviCVLlkifMzIyxCOPPCK8vLyEl5eXePLJJ6Wnk4QQYtKkSWLLli1izZo1wsPDQ/j5+YmxY8eK559/Xu+J1Yq/rQceeEB8/fXXYsCAAdL4xYsXS8fLcePGibfeekvMnz9fCFF1H+jYsaP0hKcQ5b933XlOdx7XaDSiY8eO0hNEdWUixF3qlpRu67HHHsOLL75Y4xNGzcGaNWtQWFgo9Rtxr8rMzISzs7PU0HPt2rXIzMzEypUrDRwZGTNdY+tDhw5JTznWR4cOHaq9NWFIP/74I95//328//77hg6lWUpISECHDh3w5JNPGmT5X375Jb755hts2LChXtPJ7rbOvSgpKQmdOnWCj49Ps01M/vzzTwQEBOCbb77BjBkzDB2Owe3evRvBwcFSp1D//fef1HaA6E6YOXMmunbtivnz5zcoMQEgPbItJ0qlstqnuahuzMzMpMb4d9ONGzcQGBiI1157rUHvDWLNCREREckKa06IiIhIVpicEBERkawwOSEiIiJZYXJCREREssLkhIiIiGSFyQkRERHJCpMTIiIikhUmJ0RERCQrTE6IiIhIVv4fvdGZgLvgK7kAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f89fc12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quality\n",
       "6    2198\n",
       "5    1457\n",
       "7     880\n",
       "8     175\n",
       "4     163\n",
       "3      20\n",
       "9       5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcc37cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f66ba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('quality', axis=1)\n",
    "y = data['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06a7899c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       6\n",
       "1       6\n",
       "2       6\n",
       "3       6\n",
       "4       6\n",
       "       ..\n",
       "4893    6\n",
       "4894    5\n",
       "4895    6\n",
       "4896    7\n",
       "4897    6\n",
       "Name: quality, Length: 4898, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ea8d067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4898 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          3      4      5      6      7      8      9\n",
       "0     False  False  False   True  False  False  False\n",
       "1     False  False  False   True  False  False  False\n",
       "2     False  False  False   True  False  False  False\n",
       "3     False  False  False   True  False  False  False\n",
       "4     False  False  False   True  False  False  False\n",
       "...     ...    ...    ...    ...    ...    ...    ...\n",
       "4893  False  False  False   True  False  False  False\n",
       "4894  False  False   True  False  False  False  False\n",
       "4895  False  False  False   True  False  False  False\n",
       "4896  False  False  False  False   True  False  False\n",
       "4897  False  False  False   True  False  False  False\n",
       "\n",
       "[4898 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.get_dummies(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea725434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quality\n",
       "6    2198\n",
       "5    1457\n",
       "7     880\n",
       "8     175\n",
       "4     163\n",
       "3      20\n",
       "9       5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10d530f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 홀드아웃 진행\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size=0.5, stratify=y_valid, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1101aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3940</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>1.272727</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.451220</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>0.507205</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3905</th>\n",
       "      <td>-0.272727</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.170732</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.783333</td>\n",
       "      <td>-0.811527</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>1.105263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>-0.363636</td>\n",
       "      <td>-0.727273</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.512195</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>-0.450000</td>\n",
       "      <td>-0.622478</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.928571</td>\n",
       "      <td>0.263158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2576</th>\n",
       "      <td>-0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>-0.285714</td>\n",
       "      <td>1.818182</td>\n",
       "      <td>1.533333</td>\n",
       "      <td>0.465706</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>1.363636</td>\n",
       "      <td>2.090909</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>-0.329268</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>-0.316667</td>\n",
       "      <td>0.138329</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>0.157895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2609</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>-0.545455</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.524390</td>\n",
       "      <td>-1.714286</td>\n",
       "      <td>-1.363636</td>\n",
       "      <td>-1.783333</td>\n",
       "      <td>-1.035159</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.857143</td>\n",
       "      <td>0.842105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2530</th>\n",
       "      <td>-0.909091</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>-1.666667</td>\n",
       "      <td>-0.439024</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>1.045455</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>-0.325072</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.363636</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>-0.085366</td>\n",
       "      <td>-0.928571</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.414986</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3948</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.090909</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.363636</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.205187</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.214286</td>\n",
       "      <td>-0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2460</th>\n",
       "      <td>-0.181818</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>1.323171</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.136364</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.019020</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>-0.631579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2938 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "3940       0.090909          1.272727     0.416667        0.451220   0.714286   \n",
       "3905      -0.272727          0.636364    -0.166667       -0.170732  -0.500000   \n",
       "53        -0.363636         -0.727273     0.250000       -0.512195   0.142857   \n",
       "2576      -0.272727          0.090909     0.666667        0.585366  -0.285714   \n",
       "1970       1.363636          2.090909     0.916667       -0.329268  -0.500000   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "2609       0.272727         -0.545455    -0.166667       -0.524390  -1.714286   \n",
       "2530      -0.909091          0.363636    -1.666667       -0.439024  -0.500000   \n",
       "1231       1.000000          1.363636     0.416667       -0.085366  -0.928571   \n",
       "3948      -1.000000         -1.090909    -0.166667        0.024390   0.142857   \n",
       "2460      -0.181818          0.636364    -0.666667        1.323171   0.142857   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide   density    pH  sulphates  \\\n",
       "3940             0.090909              0.216667  0.507205 -0.05   0.142857   \n",
       "3905             0.000000             -0.783333 -0.811527  0.40   1.071429   \n",
       "53               0.227273             -0.450000 -0.622478  0.70  -0.928571   \n",
       "2576             1.818182              1.533333  0.465706  0.05   0.142857   \n",
       "1970            -0.090909             -0.316667  0.138329  0.30  -0.071429   \n",
       "...                   ...                   ...       ...   ...        ...   \n",
       "2609            -1.363636             -1.783333 -1.035159  0.15  -0.857143   \n",
       "2530             1.045455              0.466667 -0.325072  1.05   0.785714   \n",
       "1231             0.272727             -0.350000 -0.414986  0.00   0.428571   \n",
       "3948            -0.363636             -0.500000  0.205187  0.75  -0.214286   \n",
       "2460            -0.136364              0.200000  1.019020 -0.45   0.285714   \n",
       "\n",
       "       alcohol  \n",
       "3940 -0.578947  \n",
       "3905  1.105263  \n",
       "53    0.263158  \n",
       "2576 -0.526316  \n",
       "1970  0.157895  \n",
       "...        ...  \n",
       "2609  0.842105  \n",
       "2530  0.052632  \n",
       "1231  1.052632  \n",
       "3948 -0.578947  \n",
       "2460 -0.631579  \n",
       "\n",
       "[2938 rows x 11 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 스케일링\n",
    "rs = RobustScaler()\n",
    "train_temp = rs.fit_transform(X_train)\n",
    "valid_temp = rs.transform(X_valid)\n",
    "test_temp = rs.transform(X_test)\n",
    "\n",
    "rs_X_train = pd.DataFrame(train_temp, columns=X_train.columns, index=X_train.index)\n",
    "rs_X_valid = pd.DataFrame(valid_temp, columns=X_valid.columns, index=X_valid.index)\n",
    "rs_X_test = pd.DataFrame(test_temp, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "rs_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73b7931a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3940</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3905</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2576</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2609</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2530</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3948</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2460</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2938 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          3      4      5      6      7      8      9\n",
       "3940  False  False   True  False  False  False  False\n",
       "3905  False  False  False  False   True  False  False\n",
       "53    False  False  False   True  False  False  False\n",
       "2576  False  False   True  False  False  False  False\n",
       "1970  False  False  False   True  False  False  False\n",
       "...     ...    ...    ...    ...    ...    ...    ...\n",
       "2609  False  False  False   True  False  False  False\n",
       "2530  False  False  False   True  False  False  False\n",
       "1231  False  False  False  False   True  False  False\n",
       "3948  False  False   True  False  False  False  False\n",
       "2460  False  False   True  False  False  False  False\n",
       "\n",
       "[2938 rows x 7 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216cae46",
   "metadata": {},
   "source": [
    "# 모델을 최고 성능에서 저장하고 중지하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "edfc6383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3454cce2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">119</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m768\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │           \u001b[38;5;34m119\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,495</span> (13.65 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,495\u001b[0m (13.65 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,495</span> (13.65 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,495\u001b[0m (13.65 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(rs_X_train.shape[1],)))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f34a77c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "552300db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.3903 - loss: 1.6919 - val_accuracy: 0.5327 - val_loss: 1.2358\n",
      "Epoch 2/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5247 - loss: 1.1966 - val_accuracy: 0.5500 - val_loss: 1.1700\n",
      "Epoch 3/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5367 - loss: 1.1472 - val_accuracy: 0.5561 - val_loss: 1.1419\n",
      "Epoch 4/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5583 - loss: 1.0831 - val_accuracy: 0.5633 - val_loss: 1.1241\n",
      "Epoch 5/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5636 - loss: 1.0541 - val_accuracy: 0.5602 - val_loss: 1.1112\n",
      "Epoch 6/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5667 - loss: 1.0493 - val_accuracy: 0.5643 - val_loss: 1.0976\n",
      "Epoch 7/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5567 - loss: 1.0426 - val_accuracy: 0.5714 - val_loss: 1.0867\n",
      "Epoch 8/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5573 - loss: 1.0270 - val_accuracy: 0.5643 - val_loss: 1.0876\n",
      "Epoch 9/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5836 - loss: 1.0062 - val_accuracy: 0.5704 - val_loss: 1.0848\n",
      "Epoch 10/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5885 - loss: 1.0025 - val_accuracy: 0.5765 - val_loss: 1.0749\n",
      "Epoch 11/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5726 - loss: 0.9848 - val_accuracy: 0.5704 - val_loss: 1.0762\n",
      "Epoch 12/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5719 - loss: 0.9714 - val_accuracy: 0.5602 - val_loss: 1.0851\n",
      "Epoch 13/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5738 - loss: 0.9863 - val_accuracy: 0.5735 - val_loss: 1.0692\n",
      "Epoch 14/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5895 - loss: 0.9800 - val_accuracy: 0.5735 - val_loss: 1.0651\n",
      "Epoch 15/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5916 - loss: 0.9556 - val_accuracy: 0.5633 - val_loss: 1.0702\n",
      "Epoch 16/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5875 - loss: 0.9795 - val_accuracy: 0.5602 - val_loss: 1.0670\n",
      "Epoch 17/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5835 - loss: 0.9581 - val_accuracy: 0.5602 - val_loss: 1.0640\n",
      "Epoch 18/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5909 - loss: 0.9618 - val_accuracy: 0.5541 - val_loss: 1.0750\n",
      "Epoch 19/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5841 - loss: 0.9541 - val_accuracy: 0.5653 - val_loss: 1.0634\n",
      "Epoch 20/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5872 - loss: 0.9375 - val_accuracy: 0.5602 - val_loss: 1.0611\n",
      "Epoch 21/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.6105 - loss: 0.9164 - val_accuracy: 0.5592 - val_loss: 1.0629\n",
      "Epoch 22/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.6089 - loss: 0.9212 - val_accuracy: 0.5694 - val_loss: 1.0566\n",
      "Epoch 23/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5983 - loss: 0.9028 - val_accuracy: 0.5663 - val_loss: 1.0622\n",
      "Epoch 24/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6055 - loss: 0.8943 - val_accuracy: 0.5653 - val_loss: 1.0592\n",
      "Epoch 25/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6347 - loss: 0.8824 - val_accuracy: 0.5633 - val_loss: 1.0660\n",
      "Epoch 26/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6066 - loss: 0.9136 - val_accuracy: 0.5622 - val_loss: 1.0720\n",
      "Epoch 27/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6032 - loss: 0.8930 - val_accuracy: 0.5684 - val_loss: 1.0728\n",
      "Epoch 28/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6178 - loss: 0.8855 - val_accuracy: 0.5643 - val_loss: 1.0580\n",
      "Epoch 29/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6029 - loss: 0.8897 - val_accuracy: 0.5663 - val_loss: 1.0648\n",
      "Epoch 30/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6131 - loss: 0.8953 - val_accuracy: 0.5561 - val_loss: 1.0744\n",
      "Epoch 31/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6146 - loss: 0.8842 - val_accuracy: 0.5592 - val_loss: 1.0755\n",
      "Epoch 32/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6128 - loss: 0.8865 - val_accuracy: 0.5653 - val_loss: 1.0673\n",
      "Epoch 33/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6309 - loss: 0.8760 - val_accuracy: 0.5622 - val_loss: 1.0693\n",
      "Epoch 34/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6386 - loss: 0.8512 - val_accuracy: 0.5796 - val_loss: 1.0718\n",
      "Epoch 35/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6302 - loss: 0.8587 - val_accuracy: 0.5684 - val_loss: 1.0689\n",
      "Epoch 36/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6424 - loss: 0.8557 - val_accuracy: 0.5622 - val_loss: 1.0805\n",
      "Epoch 37/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6399 - loss: 0.8637 - val_accuracy: 0.5663 - val_loss: 1.0805\n",
      "Epoch 38/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.6358 - loss: 0.8793 - val_accuracy: 0.5694 - val_loss: 1.0812\n",
      "Epoch 39/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6374 - loss: 0.8392 - val_accuracy: 0.5704 - val_loss: 1.0737\n",
      "Epoch 40/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6391 - loss: 0.8534 - val_accuracy: 0.5684 - val_loss: 1.0842\n",
      "Epoch 41/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6487 - loss: 0.8371 - val_accuracy: 0.5755 - val_loss: 1.0912\n",
      "Epoch 42/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6379 - loss: 0.8343 - val_accuracy: 0.5653 - val_loss: 1.0707\n",
      "Epoch 43/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6546 - loss: 0.8321 - val_accuracy: 0.5429 - val_loss: 1.1098\n",
      "Epoch 44/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6410 - loss: 0.8323 - val_accuracy: 0.5643 - val_loss: 1.0736\n",
      "Epoch 45/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6437 - loss: 0.8243 - val_accuracy: 0.5714 - val_loss: 1.0851\n",
      "Epoch 46/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6542 - loss: 0.8164 - val_accuracy: 0.5673 - val_loss: 1.0822\n",
      "Epoch 47/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6484 - loss: 0.8213 - val_accuracy: 0.5633 - val_loss: 1.1015\n",
      "Epoch 48/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6662 - loss: 0.7867 - val_accuracy: 0.5663 - val_loss: 1.0742\n",
      "Epoch 49/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6517 - loss: 0.8284 - val_accuracy: 0.5592 - val_loss: 1.0875\n",
      "Epoch 50/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6638 - loss: 0.7952 - val_accuracy: 0.5714 - val_loss: 1.1092\n",
      "Epoch 51/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6694 - loss: 0.7877 - val_accuracy: 0.5714 - val_loss: 1.1011\n",
      "Epoch 52/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6766 - loss: 0.7701 - val_accuracy: 0.5724 - val_loss: 1.0982\n",
      "Epoch 53/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6571 - loss: 0.8145 - val_accuracy: 0.5796 - val_loss: 1.0945\n",
      "Epoch 54/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6780 - loss: 0.7879 - val_accuracy: 0.5755 - val_loss: 1.1004\n",
      "Epoch 55/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6728 - loss: 0.7766 - val_accuracy: 0.5469 - val_loss: 1.1021\n",
      "Epoch 56/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6723 - loss: 0.7905 - val_accuracy: 0.5663 - val_loss: 1.1001\n",
      "Epoch 57/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6850 - loss: 0.7586 - val_accuracy: 0.5602 - val_loss: 1.0945\n",
      "Epoch 58/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6766 - loss: 0.7677 - val_accuracy: 0.5714 - val_loss: 1.1063\n",
      "Epoch 59/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6859 - loss: 0.7535 - val_accuracy: 0.5694 - val_loss: 1.1061\n",
      "Epoch 60/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6598 - loss: 0.7916 - val_accuracy: 0.5663 - val_loss: 1.1123\n",
      "Epoch 61/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6876 - loss: 0.7595 - val_accuracy: 0.5663 - val_loss: 1.1106\n",
      "Epoch 62/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7020 - loss: 0.7377 - val_accuracy: 0.5684 - val_loss: 1.1195\n",
      "Epoch 63/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6867 - loss: 0.7479 - val_accuracy: 0.5765 - val_loss: 1.1085\n",
      "Epoch 64/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6978 - loss: 0.7528 - val_accuracy: 0.5612 - val_loss: 1.1055\n",
      "Epoch 65/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6895 - loss: 0.7513 - val_accuracy: 0.5837 - val_loss: 1.1259\n",
      "Epoch 66/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7010 - loss: 0.7248 - val_accuracy: 0.5714 - val_loss: 1.1550\n",
      "Epoch 67/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6980 - loss: 0.7367 - val_accuracy: 0.5673 - val_loss: 1.1375\n",
      "Epoch 68/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6916 - loss: 0.7385 - val_accuracy: 0.5714 - val_loss: 1.1219\n",
      "Epoch 69/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7100 - loss: 0.7232 - val_accuracy: 0.5684 - val_loss: 1.1421\n",
      "Epoch 70/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6984 - loss: 0.7327 - val_accuracy: 0.5694 - val_loss: 1.1313\n",
      "Epoch 71/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6966 - loss: 0.7332 - val_accuracy: 0.5704 - val_loss: 1.1326\n",
      "Epoch 72/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7038 - loss: 0.7100 - val_accuracy: 0.5673 - val_loss: 1.1419\n",
      "Epoch 73/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7146 - loss: 0.7027 - val_accuracy: 0.5653 - val_loss: 1.1445\n",
      "Epoch 74/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7004 - loss: 0.7177 - val_accuracy: 0.5653 - val_loss: 1.1364\n",
      "Epoch 75/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7203 - loss: 0.6939 - val_accuracy: 0.5776 - val_loss: 1.1360\n",
      "Epoch 76/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6929 - loss: 0.7379 - val_accuracy: 0.5602 - val_loss: 1.1411\n",
      "Epoch 77/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7042 - loss: 0.7060 - val_accuracy: 0.5592 - val_loss: 1.1465\n",
      "Epoch 78/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7297 - loss: 0.6693 - val_accuracy: 0.5857 - val_loss: 1.1469\n",
      "Epoch 79/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7153 - loss: 0.7038 - val_accuracy: 0.5694 - val_loss: 1.1665\n",
      "Epoch 80/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6917 - loss: 0.7083 - val_accuracy: 0.5745 - val_loss: 1.1459\n",
      "Epoch 81/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7114 - loss: 0.6886 - val_accuracy: 0.5786 - val_loss: 1.1643\n",
      "Epoch 82/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6964 - loss: 0.7159 - val_accuracy: 0.5714 - val_loss: 1.1802\n",
      "Epoch 83/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7302 - loss: 0.6751 - val_accuracy: 0.5643 - val_loss: 1.1760\n",
      "Epoch 84/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7089 - loss: 0.7009 - val_accuracy: 0.5694 - val_loss: 1.1696\n",
      "Epoch 85/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7234 - loss: 0.6904 - val_accuracy: 0.5704 - val_loss: 1.1634\n",
      "Epoch 86/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7231 - loss: 0.6803 - val_accuracy: 0.5684 - val_loss: 1.1634\n",
      "Epoch 87/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7385 - loss: 0.6596 - val_accuracy: 0.5592 - val_loss: 1.1780\n",
      "Epoch 88/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7410 - loss: 0.6718 - val_accuracy: 0.5633 - val_loss: 1.1735\n",
      "Epoch 89/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7237 - loss: 0.6833 - val_accuracy: 0.5694 - val_loss: 1.1727\n",
      "Epoch 90/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7263 - loss: 0.6771 - val_accuracy: 0.5612 - val_loss: 1.1755\n",
      "Epoch 91/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7295 - loss: 0.6535 - val_accuracy: 0.5612 - val_loss: 1.1932\n",
      "Epoch 92/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7126 - loss: 0.6895 - val_accuracy: 0.5724 - val_loss: 1.1686\n",
      "Epoch 93/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7481 - loss: 0.6486 - val_accuracy: 0.5816 - val_loss: 1.1766\n",
      "Epoch 94/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7439 - loss: 0.6364 - val_accuracy: 0.5582 - val_loss: 1.2125\n",
      "Epoch 95/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7370 - loss: 0.6708 - val_accuracy: 0.5735 - val_loss: 1.2099\n",
      "Epoch 96/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7308 - loss: 0.6445 - val_accuracy: 0.5724 - val_loss: 1.2011\n",
      "Epoch 97/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7532 - loss: 0.6258 - val_accuracy: 0.5765 - val_loss: 1.1884\n",
      "Epoch 98/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7473 - loss: 0.6199 - val_accuracy: 0.5684 - val_loss: 1.2028\n",
      "Epoch 99/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7312 - loss: 0.6527 - val_accuracy: 0.5745 - val_loss: 1.2099\n",
      "Epoch 100/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7358 - loss: 0.6414 - val_accuracy: 0.5694 - val_loss: 1.2295\n",
      "Epoch 101/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7489 - loss: 0.6288 - val_accuracy: 0.5439 - val_loss: 1.2521\n",
      "Epoch 102/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7481 - loss: 0.6279 - val_accuracy: 0.5653 - val_loss: 1.2190\n",
      "Epoch 103/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7512 - loss: 0.6298 - val_accuracy: 0.5541 - val_loss: 1.2376\n",
      "Epoch 104/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7539 - loss: 0.6188 - val_accuracy: 0.5541 - val_loss: 1.2181\n",
      "Epoch 105/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7509 - loss: 0.6085 - val_accuracy: 0.5724 - val_loss: 1.2265\n",
      "Epoch 106/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7443 - loss: 0.6147 - val_accuracy: 0.5724 - val_loss: 1.2302\n",
      "Epoch 107/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7538 - loss: 0.6260 - val_accuracy: 0.5551 - val_loss: 1.2297\n",
      "Epoch 108/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7594 - loss: 0.6046 - val_accuracy: 0.5673 - val_loss: 1.2405\n",
      "Epoch 109/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7451 - loss: 0.6216 - val_accuracy: 0.5684 - val_loss: 1.2596\n",
      "Epoch 110/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7399 - loss: 0.6146 - val_accuracy: 0.5612 - val_loss: 1.2395\n",
      "Epoch 111/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7487 - loss: 0.6215 - val_accuracy: 0.5633 - val_loss: 1.2377\n",
      "Epoch 112/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7559 - loss: 0.6111 - val_accuracy: 0.5684 - val_loss: 1.2343\n",
      "Epoch 113/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7638 - loss: 0.5990 - val_accuracy: 0.5551 - val_loss: 1.2450\n",
      "Epoch 114/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7614 - loss: 0.5927 - val_accuracy: 0.5653 - val_loss: 1.2742\n",
      "Epoch 115/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7590 - loss: 0.6044 - val_accuracy: 0.5714 - val_loss: 1.2657\n",
      "Epoch 116/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7662 - loss: 0.5845 - val_accuracy: 0.5816 - val_loss: 1.2552\n",
      "Epoch 117/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7630 - loss: 0.5821 - val_accuracy: 0.5776 - val_loss: 1.2550\n",
      "Epoch 118/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7687 - loss: 0.5918 - val_accuracy: 0.5633 - val_loss: 1.2717\n",
      "Epoch 119/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7563 - loss: 0.6106 - val_accuracy: 0.5633 - val_loss: 1.2961\n",
      "Epoch 120/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7580 - loss: 0.5847 - val_accuracy: 0.5714 - val_loss: 1.2643\n",
      "Epoch 121/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7659 - loss: 0.5836 - val_accuracy: 0.5684 - val_loss: 1.2732\n",
      "Epoch 122/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7786 - loss: 0.5739 - val_accuracy: 0.5653 - val_loss: 1.3002\n",
      "Epoch 123/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7708 - loss: 0.5702 - val_accuracy: 0.5724 - val_loss: 1.3021\n",
      "Epoch 124/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7672 - loss: 0.5820 - val_accuracy: 0.5694 - val_loss: 1.2767\n",
      "Epoch 125/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7648 - loss: 0.5791 - val_accuracy: 0.5786 - val_loss: 1.2825\n",
      "Epoch 126/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7679 - loss: 0.5806 - val_accuracy: 0.5745 - val_loss: 1.2814\n",
      "Epoch 127/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7826 - loss: 0.5592 - val_accuracy: 0.5796 - val_loss: 1.3303\n",
      "Epoch 128/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7660 - loss: 0.5842 - val_accuracy: 0.5704 - val_loss: 1.3050\n",
      "Epoch 129/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7731 - loss: 0.5608 - val_accuracy: 0.5684 - val_loss: 1.2995\n",
      "Epoch 130/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7757 - loss: 0.5582 - val_accuracy: 0.5714 - val_loss: 1.2974\n",
      "Epoch 131/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7788 - loss: 0.5583 - val_accuracy: 0.5786 - val_loss: 1.3081\n",
      "Epoch 132/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7871 - loss: 0.5425 - val_accuracy: 0.5714 - val_loss: 1.3059\n",
      "Epoch 133/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7771 - loss: 0.5537 - val_accuracy: 0.5643 - val_loss: 1.3234\n",
      "Epoch 134/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7836 - loss: 0.5704 - val_accuracy: 0.5694 - val_loss: 1.3141\n",
      "Epoch 135/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7951 - loss: 0.5274 - val_accuracy: 0.5694 - val_loss: 1.3237\n",
      "Epoch 136/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7934 - loss: 0.5387 - val_accuracy: 0.5582 - val_loss: 1.3449\n",
      "Epoch 137/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7793 - loss: 0.5551 - val_accuracy: 0.5786 - val_loss: 1.3371\n",
      "Epoch 138/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7975 - loss: 0.5365 - val_accuracy: 0.5714 - val_loss: 1.3667\n",
      "Epoch 139/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7783 - loss: 0.5426 - val_accuracy: 0.5439 - val_loss: 1.3685\n",
      "Epoch 140/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7885 - loss: 0.5530 - val_accuracy: 0.5796 - val_loss: 1.3399\n",
      "Epoch 141/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7978 - loss: 0.5131 - val_accuracy: 0.5959 - val_loss: 1.3368\n",
      "Epoch 142/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7877 - loss: 0.5298 - val_accuracy: 0.5663 - val_loss: 1.3774\n",
      "Epoch 143/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7895 - loss: 0.5426 - val_accuracy: 0.5755 - val_loss: 1.3555\n",
      "Epoch 144/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7887 - loss: 0.5246 - val_accuracy: 0.5745 - val_loss: 1.4040\n",
      "Epoch 145/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7841 - loss: 0.5461 - val_accuracy: 0.5755 - val_loss: 1.3627\n",
      "Epoch 146/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7955 - loss: 0.5198 - val_accuracy: 0.5684 - val_loss: 1.3909\n",
      "Epoch 147/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8024 - loss: 0.5115 - val_accuracy: 0.5612 - val_loss: 1.3719\n",
      "Epoch 148/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7963 - loss: 0.5163 - val_accuracy: 0.5857 - val_loss: 1.3596\n",
      "Epoch 149/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8034 - loss: 0.4993 - val_accuracy: 0.5745 - val_loss: 1.3929\n",
      "Epoch 150/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7958 - loss: 0.5073 - val_accuracy: 0.5908 - val_loss: 1.3744\n",
      "Epoch 151/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7938 - loss: 0.5235 - val_accuracy: 0.5633 - val_loss: 1.3929\n",
      "Epoch 152/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8034 - loss: 0.5143 - val_accuracy: 0.5571 - val_loss: 1.4186\n",
      "Epoch 153/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8031 - loss: 0.5021 - val_accuracy: 0.5745 - val_loss: 1.4211\n",
      "Epoch 154/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8032 - loss: 0.4934 - val_accuracy: 0.5827 - val_loss: 1.3889\n",
      "Epoch 155/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8152 - loss: 0.4906 - val_accuracy: 0.5653 - val_loss: 1.4468\n",
      "Epoch 156/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8097 - loss: 0.4802 - val_accuracy: 0.5806 - val_loss: 1.4184\n",
      "Epoch 157/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7947 - loss: 0.5106 - val_accuracy: 0.5796 - val_loss: 1.4453\n",
      "Epoch 158/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8063 - loss: 0.5075 - val_accuracy: 0.5694 - val_loss: 1.4310\n",
      "Epoch 159/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8054 - loss: 0.5173 - val_accuracy: 0.5857 - val_loss: 1.4204\n",
      "Epoch 160/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8129 - loss: 0.4825 - val_accuracy: 0.5704 - val_loss: 1.4500\n",
      "Epoch 161/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8078 - loss: 0.4888 - val_accuracy: 0.5735 - val_loss: 1.4583\n",
      "Epoch 162/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7989 - loss: 0.4928 - val_accuracy: 0.5776 - val_loss: 1.4354\n",
      "Epoch 163/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8131 - loss: 0.4785 - val_accuracy: 0.5643 - val_loss: 1.4593\n",
      "Epoch 164/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8117 - loss: 0.4824 - val_accuracy: 0.5776 - val_loss: 1.4392\n",
      "Epoch 165/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8176 - loss: 0.4732 - val_accuracy: 0.5541 - val_loss: 1.4513\n",
      "Epoch 166/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8115 - loss: 0.4817 - val_accuracy: 0.5724 - val_loss: 1.4564\n",
      "Epoch 167/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8134 - loss: 0.4750 - val_accuracy: 0.5755 - val_loss: 1.4682\n",
      "Epoch 168/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8206 - loss: 0.4740 - val_accuracy: 0.5827 - val_loss: 1.4781\n",
      "Epoch 169/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8134 - loss: 0.4851 - val_accuracy: 0.5796 - val_loss: 1.4965\n",
      "Epoch 170/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8020 - loss: 0.4836 - val_accuracy: 0.5776 - val_loss: 1.4835\n",
      "Epoch 171/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8247 - loss: 0.4546 - val_accuracy: 0.5643 - val_loss: 1.5070\n",
      "Epoch 172/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8223 - loss: 0.4690 - val_accuracy: 0.5827 - val_loss: 1.4990\n",
      "Epoch 173/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8217 - loss: 0.4714 - val_accuracy: 0.5776 - val_loss: 1.4816\n",
      "Epoch 174/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8123 - loss: 0.4727 - val_accuracy: 0.5765 - val_loss: 1.4695\n",
      "Epoch 175/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8380 - loss: 0.4307 - val_accuracy: 0.5796 - val_loss: 1.5034\n",
      "Epoch 176/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8310 - loss: 0.4537 - val_accuracy: 0.5704 - val_loss: 1.5012\n",
      "Epoch 177/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8233 - loss: 0.4594 - val_accuracy: 0.5786 - val_loss: 1.5440\n",
      "Epoch 178/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8271 - loss: 0.4571 - val_accuracy: 0.5796 - val_loss: 1.5036\n",
      "Epoch 179/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8245 - loss: 0.4450 - val_accuracy: 0.5918 - val_loss: 1.4876\n",
      "Epoch 180/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8355 - loss: 0.4403 - val_accuracy: 0.5837 - val_loss: 1.5290\n",
      "Epoch 181/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8284 - loss: 0.4532 - val_accuracy: 0.5612 - val_loss: 1.5546\n",
      "Epoch 182/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8220 - loss: 0.4535 - val_accuracy: 0.5694 - val_loss: 1.5438\n",
      "Epoch 183/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8244 - loss: 0.4716 - val_accuracy: 0.5816 - val_loss: 1.5339\n",
      "Epoch 184/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8408 - loss: 0.4446 - val_accuracy: 0.5592 - val_loss: 1.5321\n",
      "Epoch 185/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8391 - loss: 0.4356 - val_accuracy: 0.5755 - val_loss: 1.5402\n",
      "Epoch 186/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8417 - loss: 0.4284 - val_accuracy: 0.5704 - val_loss: 1.5342\n",
      "Epoch 187/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8421 - loss: 0.4354 - val_accuracy: 0.5735 - val_loss: 1.5498\n",
      "Epoch 188/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8397 - loss: 0.4341 - val_accuracy: 0.5714 - val_loss: 1.5635\n",
      "Epoch 189/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8097 - loss: 0.4716 - val_accuracy: 0.5929 - val_loss: 1.5721\n",
      "Epoch 190/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8344 - loss: 0.4418 - val_accuracy: 0.5786 - val_loss: 1.5606\n",
      "Epoch 191/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8302 - loss: 0.4559 - val_accuracy: 0.5724 - val_loss: 1.5859\n",
      "Epoch 192/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8376 - loss: 0.4302 - val_accuracy: 0.5745 - val_loss: 1.5639\n",
      "Epoch 193/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8288 - loss: 0.4387 - val_accuracy: 0.5745 - val_loss: 1.6074\n",
      "Epoch 194/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8343 - loss: 0.4350 - val_accuracy: 0.5786 - val_loss: 1.5934\n",
      "Epoch 195/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8364 - loss: 0.4406 - val_accuracy: 0.5704 - val_loss: 1.5964\n",
      "Epoch 196/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8312 - loss: 0.4265 - val_accuracy: 0.5724 - val_loss: 1.6280\n",
      "Epoch 197/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8334 - loss: 0.4251 - val_accuracy: 0.5786 - val_loss: 1.5960\n",
      "Epoch 198/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8357 - loss: 0.4378 - val_accuracy: 0.5724 - val_loss: 1.6252\n",
      "Epoch 199/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8544 - loss: 0.3935 - val_accuracy: 0.5796 - val_loss: 1.6165\n",
      "Epoch 200/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8489 - loss: 0.4057 - val_accuracy: 0.5724 - val_loss: 1.6339\n",
      "Epoch 201/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8422 - loss: 0.4080 - val_accuracy: 0.5806 - val_loss: 1.6656\n",
      "Epoch 202/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8389 - loss: 0.4040 - val_accuracy: 0.5714 - val_loss: 1.6507\n",
      "Epoch 203/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8232 - loss: 0.4524 - val_accuracy: 0.5878 - val_loss: 1.6392\n",
      "Epoch 204/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8573 - loss: 0.3840 - val_accuracy: 0.5776 - val_loss: 1.6292\n",
      "Epoch 205/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8460 - loss: 0.3965 - val_accuracy: 0.5776 - val_loss: 1.6259\n",
      "Epoch 206/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8485 - loss: 0.3928 - val_accuracy: 0.5765 - val_loss: 1.6486\n",
      "Epoch 207/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8507 - loss: 0.3911 - val_accuracy: 0.5735 - val_loss: 1.6449\n",
      "Epoch 208/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8539 - loss: 0.4017 - val_accuracy: 0.5745 - val_loss: 1.6673\n",
      "Epoch 209/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8463 - loss: 0.4106 - val_accuracy: 0.5673 - val_loss: 1.6506\n",
      "Epoch 210/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8478 - loss: 0.4016 - val_accuracy: 0.5765 - val_loss: 1.7207\n",
      "Epoch 211/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8561 - loss: 0.3871 - val_accuracy: 0.5816 - val_loss: 1.6829\n",
      "Epoch 212/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8630 - loss: 0.3795 - val_accuracy: 0.5806 - val_loss: 1.6925\n",
      "Epoch 213/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8505 - loss: 0.4057 - val_accuracy: 0.5806 - val_loss: 1.6812\n",
      "Epoch 214/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8698 - loss: 0.3738 - val_accuracy: 0.5806 - val_loss: 1.7263\n",
      "Epoch 215/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8682 - loss: 0.3860 - val_accuracy: 0.5837 - val_loss: 1.7029\n",
      "Epoch 216/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8514 - loss: 0.3934 - val_accuracy: 0.5796 - val_loss: 1.7044\n",
      "Epoch 217/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8547 - loss: 0.3830 - val_accuracy: 0.5786 - val_loss: 1.7408\n",
      "Epoch 218/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8416 - loss: 0.4051 - val_accuracy: 0.5786 - val_loss: 1.7322\n",
      "Epoch 219/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8598 - loss: 0.3867 - val_accuracy: 0.5908 - val_loss: 1.7433\n",
      "Epoch 220/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8498 - loss: 0.3927 - val_accuracy: 0.5847 - val_loss: 1.7287\n",
      "Epoch 221/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8618 - loss: 0.3802 - val_accuracy: 0.5643 - val_loss: 1.7124\n",
      "Epoch 222/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8552 - loss: 0.3794 - val_accuracy: 0.5592 - val_loss: 1.7665\n",
      "Epoch 223/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8660 - loss: 0.3653 - val_accuracy: 0.5796 - val_loss: 1.7486\n",
      "Epoch 224/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8577 - loss: 0.3796 - val_accuracy: 0.5806 - val_loss: 1.7721\n",
      "Epoch 225/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8438 - loss: 0.3984 - val_accuracy: 0.5786 - val_loss: 1.7723\n",
      "Epoch 226/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8595 - loss: 0.3690 - val_accuracy: 0.5898 - val_loss: 1.7751\n",
      "Epoch 227/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8586 - loss: 0.3787 - val_accuracy: 0.5796 - val_loss: 1.7972\n",
      "Epoch 228/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8620 - loss: 0.3699 - val_accuracy: 0.5806 - val_loss: 1.7791\n",
      "Epoch 229/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.8649 - loss: 0.3641 - val_accuracy: 0.5878 - val_loss: 1.7794\n",
      "Epoch 230/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8690 - loss: 0.3681 - val_accuracy: 0.5786 - val_loss: 1.7958\n",
      "Epoch 231/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8691 - loss: 0.3592 - val_accuracy: 0.5653 - val_loss: 1.8395\n",
      "Epoch 232/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8543 - loss: 0.3863 - val_accuracy: 0.5827 - val_loss: 1.8215\n",
      "Epoch 233/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8650 - loss: 0.3701 - val_accuracy: 0.5765 - val_loss: 1.8583\n",
      "Epoch 234/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8622 - loss: 0.3700 - val_accuracy: 0.5929 - val_loss: 1.8043\n",
      "Epoch 235/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8645 - loss: 0.3606 - val_accuracy: 0.5816 - val_loss: 1.7862\n",
      "Epoch 236/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8663 - loss: 0.3631 - val_accuracy: 0.5633 - val_loss: 1.8804\n",
      "Epoch 237/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8622 - loss: 0.3648 - val_accuracy: 0.5765 - val_loss: 1.8353\n",
      "Epoch 238/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8715 - loss: 0.3542 - val_accuracy: 0.5643 - val_loss: 1.8439\n",
      "Epoch 239/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8426 - loss: 0.3913 - val_accuracy: 0.5673 - val_loss: 1.8927\n",
      "Epoch 240/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8629 - loss: 0.3593 - val_accuracy: 0.5918 - val_loss: 1.8656\n",
      "Epoch 241/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8847 - loss: 0.3404 - val_accuracy: 0.5714 - val_loss: 1.8840\n",
      "Epoch 242/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8881 - loss: 0.3286 - val_accuracy: 0.5837 - val_loss: 1.8577\n",
      "Epoch 243/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8815 - loss: 0.3381 - val_accuracy: 0.5867 - val_loss: 1.8898\n",
      "Epoch 244/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8736 - loss: 0.3419 - val_accuracy: 0.5796 - val_loss: 1.9280\n",
      "Epoch 245/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8585 - loss: 0.3713 - val_accuracy: 0.5847 - val_loss: 1.8744\n",
      "Epoch 246/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8799 - loss: 0.3428 - val_accuracy: 0.5459 - val_loss: 1.9227\n",
      "Epoch 247/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8702 - loss: 0.3498 - val_accuracy: 0.5939 - val_loss: 1.8927\n",
      "Epoch 248/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8663 - loss: 0.3504 - val_accuracy: 0.5816 - val_loss: 1.9323\n",
      "Epoch 249/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8730 - loss: 0.3412 - val_accuracy: 0.5908 - val_loss: 1.9103\n",
      "Epoch 250/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8844 - loss: 0.3472 - val_accuracy: 0.5531 - val_loss: 1.9264\n",
      "Epoch 251/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8682 - loss: 0.3431 - val_accuracy: 0.5929 - val_loss: 1.9271\n",
      "Epoch 252/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8764 - loss: 0.3431 - val_accuracy: 0.5673 - val_loss: 1.9473\n",
      "Epoch 253/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8719 - loss: 0.3470 - val_accuracy: 0.5735 - val_loss: 1.9486\n",
      "Epoch 254/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8776 - loss: 0.3306 - val_accuracy: 0.5806 - val_loss: 1.9208\n",
      "Epoch 255/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8944 - loss: 0.3186 - val_accuracy: 0.5786 - val_loss: 1.9713\n",
      "Epoch 256/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8876 - loss: 0.3307 - val_accuracy: 0.5684 - val_loss: 1.9655\n",
      "Epoch 257/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8805 - loss: 0.3336 - val_accuracy: 0.5735 - val_loss: 1.9879\n",
      "Epoch 258/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8789 - loss: 0.3296 - val_accuracy: 0.5755 - val_loss: 1.9930\n",
      "Epoch 259/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8753 - loss: 0.3312 - val_accuracy: 0.5867 - val_loss: 1.9645\n",
      "Epoch 260/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9021 - loss: 0.3073 - val_accuracy: 0.5847 - val_loss: 1.9535\n",
      "Epoch 261/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8839 - loss: 0.3135 - val_accuracy: 0.5765 - val_loss: 2.0168\n",
      "Epoch 262/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8845 - loss: 0.3222 - val_accuracy: 0.5714 - val_loss: 2.0015\n",
      "Epoch 263/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8792 - loss: 0.3207 - val_accuracy: 0.5735 - val_loss: 1.9983\n",
      "Epoch 264/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8909 - loss: 0.3186 - val_accuracy: 0.5714 - val_loss: 2.0159\n",
      "Epoch 265/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8782 - loss: 0.3230 - val_accuracy: 0.5724 - val_loss: 2.0477\n",
      "Epoch 266/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8889 - loss: 0.3274 - val_accuracy: 0.5898 - val_loss: 2.0278\n",
      "Epoch 267/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8886 - loss: 0.3160 - val_accuracy: 0.5745 - val_loss: 2.0011\n",
      "Epoch 268/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8804 - loss: 0.3208 - val_accuracy: 0.5694 - val_loss: 2.0382\n",
      "Epoch 269/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9043 - loss: 0.2888 - val_accuracy: 0.5847 - val_loss: 2.0032\n",
      "Epoch 270/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8966 - loss: 0.2984 - val_accuracy: 0.5796 - val_loss: 2.0399\n",
      "Epoch 271/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8982 - loss: 0.3159 - val_accuracy: 0.5837 - val_loss: 2.0410\n",
      "Epoch 272/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8996 - loss: 0.3014 - val_accuracy: 0.5878 - val_loss: 2.0653\n",
      "Epoch 273/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9010 - loss: 0.3006 - val_accuracy: 0.5765 - val_loss: 2.0393\n",
      "Epoch 274/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8978 - loss: 0.3019 - val_accuracy: 0.5796 - val_loss: 2.0556\n",
      "Epoch 275/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8945 - loss: 0.2947 - val_accuracy: 0.5806 - val_loss: 2.1202\n",
      "Epoch 276/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8806 - loss: 0.3262 - val_accuracy: 0.5786 - val_loss: 2.0230\n",
      "Epoch 277/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8975 - loss: 0.2944 - val_accuracy: 0.5929 - val_loss: 2.0784\n",
      "Epoch 278/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8766 - loss: 0.3334 - val_accuracy: 0.5724 - val_loss: 2.1516\n",
      "Epoch 279/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8930 - loss: 0.3064 - val_accuracy: 0.5816 - val_loss: 2.0876\n",
      "Epoch 280/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8879 - loss: 0.3147 - val_accuracy: 0.5643 - val_loss: 2.1299\n",
      "Epoch 281/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9011 - loss: 0.2913 - val_accuracy: 0.5745 - val_loss: 2.1209\n",
      "Epoch 282/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8881 - loss: 0.3051 - val_accuracy: 0.5765 - val_loss: 2.1538\n",
      "Epoch 283/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8757 - loss: 0.3179 - val_accuracy: 0.5724 - val_loss: 2.1264\n",
      "Epoch 284/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9063 - loss: 0.2939 - val_accuracy: 0.5755 - val_loss: 2.1947\n",
      "Epoch 285/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8993 - loss: 0.2876 - val_accuracy: 0.5816 - val_loss: 2.1394\n",
      "Epoch 286/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8919 - loss: 0.2973 - val_accuracy: 0.5745 - val_loss: 2.1180\n",
      "Epoch 287/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8843 - loss: 0.3128 - val_accuracy: 0.5633 - val_loss: 2.1496\n",
      "Epoch 288/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9006 - loss: 0.2798 - val_accuracy: 0.5888 - val_loss: 2.1160\n",
      "Epoch 289/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9130 - loss: 0.2729 - val_accuracy: 0.5898 - val_loss: 2.1864\n",
      "Epoch 290/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8968 - loss: 0.2910 - val_accuracy: 0.5745 - val_loss: 2.1750\n",
      "Epoch 291/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8962 - loss: 0.2902 - val_accuracy: 0.5806 - val_loss: 2.1945\n",
      "Epoch 292/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9006 - loss: 0.2787 - val_accuracy: 0.5837 - val_loss: 2.1990\n",
      "Epoch 293/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9025 - loss: 0.2715 - val_accuracy: 0.5755 - val_loss: 2.1809\n",
      "Epoch 294/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9034 - loss: 0.2811 - val_accuracy: 0.5449 - val_loss: 2.2974\n",
      "Epoch 295/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8899 - loss: 0.2989 - val_accuracy: 0.5704 - val_loss: 2.2317\n",
      "Epoch 296/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8912 - loss: 0.2836 - val_accuracy: 0.5765 - val_loss: 2.1965\n",
      "Epoch 297/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8951 - loss: 0.2986 - val_accuracy: 0.5765 - val_loss: 2.2319\n",
      "Epoch 298/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8992 - loss: 0.2834 - val_accuracy: 0.5888 - val_loss: 2.2300\n",
      "Epoch 299/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8926 - loss: 0.2954 - val_accuracy: 0.5837 - val_loss: 2.2506\n",
      "Epoch 300/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8905 - loss: 0.2908 - val_accuracy: 0.5765 - val_loss: 2.2144\n",
      "Epoch 301/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8963 - loss: 0.2826 - val_accuracy: 0.5694 - val_loss: 2.2319\n",
      "Epoch 302/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9036 - loss: 0.2767 - val_accuracy: 0.5837 - val_loss: 2.2647\n",
      "Epoch 303/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9140 - loss: 0.2724 - val_accuracy: 0.5796 - val_loss: 2.2557\n",
      "Epoch 304/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8956 - loss: 0.2758 - val_accuracy: 0.5673 - val_loss: 2.2340\n",
      "Epoch 305/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9099 - loss: 0.2618 - val_accuracy: 0.5714 - val_loss: 2.2860\n",
      "Epoch 306/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8916 - loss: 0.2936 - val_accuracy: 0.5918 - val_loss: 2.2708\n",
      "Epoch 307/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9003 - loss: 0.2717 - val_accuracy: 0.5704 - val_loss: 2.3070\n",
      "Epoch 308/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9031 - loss: 0.2670 - val_accuracy: 0.5867 - val_loss: 2.2809\n",
      "Epoch 309/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9105 - loss: 0.2501 - val_accuracy: 0.5816 - val_loss: 2.3094\n",
      "Epoch 310/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9025 - loss: 0.2620 - val_accuracy: 0.5724 - val_loss: 2.3631\n",
      "Epoch 311/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8975 - loss: 0.2894 - val_accuracy: 0.5694 - val_loss: 2.3756\n",
      "Epoch 312/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9093 - loss: 0.2604 - val_accuracy: 0.5765 - val_loss: 2.3046\n",
      "Epoch 313/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9054 - loss: 0.2754 - val_accuracy: 0.5857 - val_loss: 2.3501\n",
      "Epoch 314/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9105 - loss: 0.2449 - val_accuracy: 0.5765 - val_loss: 2.3196\n",
      "Epoch 315/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9185 - loss: 0.2439 - val_accuracy: 0.5918 - val_loss: 2.3416\n",
      "Epoch 316/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9033 - loss: 0.2754 - val_accuracy: 0.5786 - val_loss: 2.3468\n",
      "Epoch 317/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9129 - loss: 0.2542 - val_accuracy: 0.5918 - val_loss: 2.3315\n",
      "Epoch 318/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9039 - loss: 0.2677 - val_accuracy: 0.5724 - val_loss: 2.3466\n",
      "Epoch 319/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9181 - loss: 0.2524 - val_accuracy: 0.5684 - val_loss: 2.3638\n",
      "Epoch 320/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9152 - loss: 0.2531 - val_accuracy: 0.5806 - val_loss: 2.3617\n",
      "Epoch 321/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8993 - loss: 0.2664 - val_accuracy: 0.5673 - val_loss: 2.4228\n",
      "Epoch 322/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9031 - loss: 0.2860 - val_accuracy: 0.5796 - val_loss: 2.4079\n",
      "Epoch 323/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9068 - loss: 0.2611 - val_accuracy: 0.5837 - val_loss: 2.3824\n",
      "Epoch 324/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9185 - loss: 0.2385 - val_accuracy: 0.5735 - val_loss: 2.3942\n",
      "Epoch 325/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9114 - loss: 0.2467 - val_accuracy: 0.5765 - val_loss: 2.4188\n",
      "Epoch 326/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9207 - loss: 0.2371 - val_accuracy: 0.5867 - val_loss: 2.3739\n",
      "Epoch 327/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9158 - loss: 0.2291 - val_accuracy: 0.5867 - val_loss: 2.4382\n",
      "Epoch 328/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9209 - loss: 0.2433 - val_accuracy: 0.5735 - val_loss: 2.4303\n",
      "Epoch 329/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9198 - loss: 0.2452 - val_accuracy: 0.5724 - val_loss: 2.4900\n",
      "Epoch 330/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9104 - loss: 0.2677 - val_accuracy: 0.5724 - val_loss: 2.4744\n",
      "Epoch 331/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9141 - loss: 0.2514 - val_accuracy: 0.5786 - val_loss: 2.4204\n",
      "Epoch 332/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9040 - loss: 0.2729 - val_accuracy: 0.5888 - val_loss: 2.4586\n",
      "Epoch 333/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9209 - loss: 0.2330 - val_accuracy: 0.5867 - val_loss: 2.4699\n",
      "Epoch 334/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9138 - loss: 0.2459 - val_accuracy: 0.5704 - val_loss: 2.4924\n",
      "Epoch 335/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9173 - loss: 0.2448 - val_accuracy: 0.5724 - val_loss: 2.4469\n",
      "Epoch 336/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9191 - loss: 0.2428 - val_accuracy: 0.5806 - val_loss: 2.4768\n",
      "Epoch 337/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9199 - loss: 0.2361 - val_accuracy: 0.5796 - val_loss: 2.4642\n",
      "Epoch 338/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9116 - loss: 0.2404 - val_accuracy: 0.5827 - val_loss: 2.4821\n",
      "Epoch 339/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9208 - loss: 0.2364 - val_accuracy: 0.5867 - val_loss: 2.4662\n",
      "Epoch 340/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9192 - loss: 0.2488 - val_accuracy: 0.5816 - val_loss: 2.5161\n",
      "Epoch 341/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9223 - loss: 0.2378 - val_accuracy: 0.5867 - val_loss: 2.5202\n",
      "Epoch 342/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9214 - loss: 0.2263 - val_accuracy: 0.5857 - val_loss: 2.5082\n",
      "Epoch 343/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9202 - loss: 0.2405 - val_accuracy: 0.5908 - val_loss: 2.5388\n",
      "Epoch 344/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9220 - loss: 0.2265 - val_accuracy: 0.5755 - val_loss: 2.5286\n",
      "Epoch 345/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9149 - loss: 0.2447 - val_accuracy: 0.5724 - val_loss: 2.5084\n",
      "Epoch 346/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9104 - loss: 0.2378 - val_accuracy: 0.5714 - val_loss: 2.5740\n",
      "Epoch 347/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9181 - loss: 0.2310 - val_accuracy: 0.5847 - val_loss: 2.5376\n",
      "Epoch 348/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9246 - loss: 0.2259 - val_accuracy: 0.5796 - val_loss: 2.5220\n",
      "Epoch 349/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9239 - loss: 0.2243 - val_accuracy: 0.5816 - val_loss: 2.6122\n",
      "Epoch 350/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9122 - loss: 0.2356 - val_accuracy: 0.5898 - val_loss: 2.5601\n",
      "Epoch 351/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9180 - loss: 0.2408 - val_accuracy: 0.5847 - val_loss: 2.6575\n",
      "Epoch 352/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9288 - loss: 0.2156 - val_accuracy: 0.5827 - val_loss: 2.6140\n",
      "Epoch 353/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9092 - loss: 0.2381 - val_accuracy: 0.5735 - val_loss: 2.6957\n",
      "Epoch 354/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9166 - loss: 0.2344 - val_accuracy: 0.5592 - val_loss: 2.7072\n",
      "Epoch 355/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9066 - loss: 0.2673 - val_accuracy: 0.5837 - val_loss: 2.6174\n",
      "Epoch 356/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9150 - loss: 0.2523 - val_accuracy: 0.5755 - val_loss: 2.6402\n",
      "Epoch 357/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9349 - loss: 0.2049 - val_accuracy: 0.5867 - val_loss: 2.6434\n",
      "Epoch 358/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9275 - loss: 0.2156 - val_accuracy: 0.5776 - val_loss: 2.6444\n",
      "Epoch 359/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9167 - loss: 0.2324 - val_accuracy: 0.5878 - val_loss: 2.6340\n",
      "Epoch 360/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9299 - loss: 0.2212 - val_accuracy: 0.5714 - val_loss: 2.6974\n",
      "Epoch 361/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9257 - loss: 0.2346 - val_accuracy: 0.5827 - val_loss: 2.6184\n",
      "Epoch 362/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9260 - loss: 0.2141 - val_accuracy: 0.5704 - val_loss: 2.6368\n",
      "Epoch 363/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9245 - loss: 0.2131 - val_accuracy: 0.5653 - val_loss: 2.6865\n",
      "Epoch 364/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9176 - loss: 0.2293 - val_accuracy: 0.5724 - val_loss: 2.6591\n",
      "Epoch 365/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9296 - loss: 0.2172 - val_accuracy: 0.5776 - val_loss: 2.6646\n",
      "Epoch 366/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9222 - loss: 0.2192 - val_accuracy: 0.5765 - val_loss: 2.6937\n",
      "Epoch 367/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9282 - loss: 0.2064 - val_accuracy: 0.5561 - val_loss: 2.7481\n",
      "Epoch 368/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9204 - loss: 0.2139 - val_accuracy: 0.5653 - val_loss: 2.6846\n",
      "Epoch 369/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9308 - loss: 0.2127 - val_accuracy: 0.5816 - val_loss: 2.6820\n",
      "Epoch 370/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9250 - loss: 0.2283 - val_accuracy: 0.5816 - val_loss: 2.7040\n",
      "Epoch 371/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9262 - loss: 0.2126 - val_accuracy: 0.5816 - val_loss: 2.7387\n",
      "Epoch 372/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9361 - loss: 0.2152 - val_accuracy: 0.5898 - val_loss: 2.7220\n",
      "Epoch 373/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9326 - loss: 0.2031 - val_accuracy: 0.5776 - val_loss: 2.8209\n",
      "Epoch 374/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9285 - loss: 0.2142 - val_accuracy: 0.5704 - val_loss: 2.7716\n",
      "Epoch 375/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9306 - loss: 0.2099 - val_accuracy: 0.5735 - val_loss: 2.7113\n",
      "Epoch 376/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9296 - loss: 0.1989 - val_accuracy: 0.5602 - val_loss: 2.7777\n",
      "Epoch 377/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9334 - loss: 0.2101 - val_accuracy: 0.5827 - val_loss: 2.7537\n",
      "Epoch 378/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9204 - loss: 0.2183 - val_accuracy: 0.5786 - val_loss: 2.8566\n",
      "Epoch 379/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9346 - loss: 0.1911 - val_accuracy: 0.5898 - val_loss: 2.8096\n",
      "Epoch 380/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9320 - loss: 0.2049 - val_accuracy: 0.5847 - val_loss: 2.8026\n",
      "Epoch 381/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9361 - loss: 0.1993 - val_accuracy: 0.5786 - val_loss: 2.8201\n",
      "Epoch 382/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9360 - loss: 0.1942 - val_accuracy: 0.5612 - val_loss: 2.8442\n",
      "Epoch 383/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9320 - loss: 0.2071 - val_accuracy: 0.5745 - val_loss: 2.8124\n",
      "Epoch 384/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9262 - loss: 0.2141 - val_accuracy: 0.5786 - val_loss: 2.8266\n",
      "Epoch 385/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9378 - loss: 0.2003 - val_accuracy: 0.5704 - val_loss: 2.8281\n",
      "Epoch 386/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9363 - loss: 0.1986 - val_accuracy: 0.5684 - val_loss: 2.8993\n",
      "Epoch 387/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9351 - loss: 0.1967 - val_accuracy: 0.5714 - val_loss: 2.8332\n",
      "Epoch 388/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9175 - loss: 0.2257 - val_accuracy: 0.5847 - val_loss: 2.8423\n",
      "Epoch 389/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9340 - loss: 0.1918 - val_accuracy: 0.5990 - val_loss: 2.8229\n",
      "Epoch 390/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9129 - loss: 0.2345 - val_accuracy: 0.5643 - val_loss: 2.8751\n",
      "Epoch 391/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9345 - loss: 0.1912 - val_accuracy: 0.5776 - val_loss: 2.8485\n",
      "Epoch 392/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9461 - loss: 0.1777 - val_accuracy: 0.5837 - val_loss: 2.8071\n",
      "Epoch 393/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9389 - loss: 0.1945 - val_accuracy: 0.5755 - val_loss: 2.8759\n",
      "Epoch 394/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9287 - loss: 0.2088 - val_accuracy: 0.5714 - val_loss: 2.9214\n",
      "Epoch 395/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9343 - loss: 0.1992 - val_accuracy: 0.5776 - val_loss: 2.9224\n",
      "Epoch 396/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9344 - loss: 0.1934 - val_accuracy: 0.5878 - val_loss: 2.9176\n",
      "Epoch 397/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9418 - loss: 0.1953 - val_accuracy: 0.5929 - val_loss: 2.9206\n",
      "Epoch 398/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9315 - loss: 0.1974 - val_accuracy: 0.5765 - val_loss: 2.9286\n",
      "Epoch 399/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9375 - loss: 0.1965 - val_accuracy: 0.5724 - val_loss: 2.9541\n",
      "Epoch 400/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9370 - loss: 0.1857 - val_accuracy: 0.5918 - val_loss: 2.9245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 401/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9377 - loss: 0.1837 - val_accuracy: 0.5765 - val_loss: 2.9184\n",
      "Epoch 402/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9386 - loss: 0.1838 - val_accuracy: 0.5827 - val_loss: 2.9110\n",
      "Epoch 403/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9387 - loss: 0.1820 - val_accuracy: 0.5867 - val_loss: 2.9475\n",
      "Epoch 404/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9377 - loss: 0.1911 - val_accuracy: 0.5745 - val_loss: 2.9858\n",
      "Epoch 405/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9244 - loss: 0.2134 - val_accuracy: 0.5918 - val_loss: 2.9681\n",
      "Epoch 406/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9238 - loss: 0.2260 - val_accuracy: 0.5776 - val_loss: 2.9855\n",
      "Epoch 407/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9420 - loss: 0.1868 - val_accuracy: 0.5796 - val_loss: 2.9509\n",
      "Epoch 408/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9458 - loss: 0.1713 - val_accuracy: 0.5929 - val_loss: 2.9647\n",
      "Epoch 409/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9425 - loss: 0.1777 - val_accuracy: 0.5847 - val_loss: 2.9922\n",
      "Epoch 410/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9454 - loss: 0.1839 - val_accuracy: 0.5857 - val_loss: 2.9643\n",
      "Epoch 411/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9381 - loss: 0.1716 - val_accuracy: 0.5786 - val_loss: 2.9562\n",
      "Epoch 412/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9484 - loss: 0.1724 - val_accuracy: 0.5837 - val_loss: 3.1018\n",
      "Epoch 413/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9294 - loss: 0.1979 - val_accuracy: 0.5735 - val_loss: 3.0247\n",
      "Epoch 414/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9385 - loss: 0.1864 - val_accuracy: 0.5898 - val_loss: 2.9914\n",
      "Epoch 415/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9398 - loss: 0.1770 - val_accuracy: 0.5582 - val_loss: 3.0744\n",
      "Epoch 416/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9401 - loss: 0.1839 - val_accuracy: 0.5796 - val_loss: 2.9838\n",
      "Epoch 417/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9308 - loss: 0.1902 - val_accuracy: 0.5755 - val_loss: 3.0219\n",
      "Epoch 418/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9416 - loss: 0.1687 - val_accuracy: 0.5633 - val_loss: 3.0428\n",
      "Epoch 419/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9452 - loss: 0.1701 - val_accuracy: 0.5878 - val_loss: 3.0352\n",
      "Epoch 420/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9358 - loss: 0.1833 - val_accuracy: 0.5908 - val_loss: 3.0584\n",
      "Epoch 421/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9521 - loss: 0.1599 - val_accuracy: 0.5857 - val_loss: 3.1117\n",
      "Epoch 422/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9401 - loss: 0.1749 - val_accuracy: 0.5929 - val_loss: 3.0886\n",
      "Epoch 423/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9372 - loss: 0.1838 - val_accuracy: 0.5827 - val_loss: 3.0578\n",
      "Epoch 424/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9446 - loss: 0.1745 - val_accuracy: 0.5878 - val_loss: 3.0759\n",
      "Epoch 425/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9446 - loss: 0.1711 - val_accuracy: 0.5847 - val_loss: 3.1306\n",
      "Epoch 426/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9340 - loss: 0.1874 - val_accuracy: 0.5796 - val_loss: 3.0595\n",
      "Epoch 427/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9391 - loss: 0.1847 - val_accuracy: 0.5776 - val_loss: 3.1336\n",
      "Epoch 428/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9285 - loss: 0.1948 - val_accuracy: 0.5827 - val_loss: 3.1629\n",
      "Epoch 429/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9230 - loss: 0.2110 - val_accuracy: 0.5786 - val_loss: 3.0903\n",
      "Epoch 430/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9327 - loss: 0.1939 - val_accuracy: 0.5837 - val_loss: 3.1128\n",
      "Epoch 431/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9549 - loss: 0.1558 - val_accuracy: 0.5847 - val_loss: 3.1442\n",
      "Epoch 432/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9464 - loss: 0.1658 - val_accuracy: 0.6041 - val_loss: 3.1324\n",
      "Epoch 433/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9386 - loss: 0.1839 - val_accuracy: 0.5612 - val_loss: 3.2089\n",
      "Epoch 434/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9477 - loss: 0.1699 - val_accuracy: 0.5908 - val_loss: 3.1312\n",
      "Epoch 435/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9423 - loss: 0.1765 - val_accuracy: 0.5847 - val_loss: 3.1634\n",
      "Epoch 436/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9430 - loss: 0.1744 - val_accuracy: 0.5867 - val_loss: 3.1936\n",
      "Epoch 437/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9431 - loss: 0.1691 - val_accuracy: 0.5827 - val_loss: 3.1810\n",
      "Epoch 438/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9511 - loss: 0.1693 - val_accuracy: 0.5796 - val_loss: 3.1629\n",
      "Epoch 439/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9491 - loss: 0.1712 - val_accuracy: 0.5918 - val_loss: 3.2045\n",
      "Epoch 440/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9441 - loss: 0.1702 - val_accuracy: 0.5888 - val_loss: 3.2590\n",
      "Epoch 441/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9460 - loss: 0.1718 - val_accuracy: 0.5847 - val_loss: 3.1967\n",
      "Epoch 442/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9537 - loss: 0.1514 - val_accuracy: 0.5765 - val_loss: 3.2726\n",
      "Epoch 443/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9553 - loss: 0.1554 - val_accuracy: 0.5837 - val_loss: 3.2167\n",
      "Epoch 444/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9468 - loss: 0.1670 - val_accuracy: 0.5878 - val_loss: 3.2245\n",
      "Epoch 445/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9570 - loss: 0.1544 - val_accuracy: 0.5827 - val_loss: 3.2789\n",
      "Epoch 446/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9475 - loss: 0.1542 - val_accuracy: 0.5735 - val_loss: 3.1988\n",
      "Epoch 447/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9493 - loss: 0.1651 - val_accuracy: 0.5571 - val_loss: 3.2824\n",
      "Epoch 448/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9554 - loss: 0.1482 - val_accuracy: 0.5837 - val_loss: 3.1988\n",
      "Epoch 449/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9574 - loss: 0.1509 - val_accuracy: 0.5765 - val_loss: 3.3250\n",
      "Epoch 450/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9509 - loss: 0.1609 - val_accuracy: 0.5704 - val_loss: 3.2405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 451/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9272 - loss: 0.1934 - val_accuracy: 0.5847 - val_loss: 3.3360\n",
      "Epoch 452/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9513 - loss: 0.1534 - val_accuracy: 0.5816 - val_loss: 3.3333\n",
      "Epoch 453/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9527 - loss: 0.1732 - val_accuracy: 0.5908 - val_loss: 3.3237\n",
      "Epoch 454/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9433 - loss: 0.1819 - val_accuracy: 0.5878 - val_loss: 3.2798\n",
      "Epoch 455/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9406 - loss: 0.1714 - val_accuracy: 0.5653 - val_loss: 3.3048\n",
      "Epoch 456/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9526 - loss: 0.1508 - val_accuracy: 0.5837 - val_loss: 3.3046\n",
      "Epoch 457/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9428 - loss: 0.1668 - val_accuracy: 0.5847 - val_loss: 3.3387\n",
      "Epoch 458/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9466 - loss: 0.1522 - val_accuracy: 0.5857 - val_loss: 3.3731\n",
      "Epoch 459/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9448 - loss: 0.1901 - val_accuracy: 0.5796 - val_loss: 3.3632\n",
      "Epoch 460/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9556 - loss: 0.1528 - val_accuracy: 0.5857 - val_loss: 3.3091\n",
      "Epoch 461/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9577 - loss: 0.1491 - val_accuracy: 0.5908 - val_loss: 3.4132\n",
      "Epoch 462/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9493 - loss: 0.1588 - val_accuracy: 0.5827 - val_loss: 3.3635\n",
      "Epoch 463/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9550 - loss: 0.1493 - val_accuracy: 0.6000 - val_loss: 3.3029\n",
      "Epoch 464/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9534 - loss: 0.1362 - val_accuracy: 0.5857 - val_loss: 3.4037\n",
      "Epoch 465/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9423 - loss: 0.1670 - val_accuracy: 0.5847 - val_loss: 3.4675\n",
      "Epoch 466/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9483 - loss: 0.1634 - val_accuracy: 0.5898 - val_loss: 3.4374\n",
      "Epoch 467/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9579 - loss: 0.1448 - val_accuracy: 0.5857 - val_loss: 3.4396\n",
      "Epoch 468/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9522 - loss: 0.1440 - val_accuracy: 0.5827 - val_loss: 3.4201\n",
      "Epoch 469/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9547 - loss: 0.1473 - val_accuracy: 0.5888 - val_loss: 3.3758\n",
      "Epoch 470/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9569 - loss: 0.1471 - val_accuracy: 0.5806 - val_loss: 3.4493\n",
      "Epoch 471/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9512 - loss: 0.1598 - val_accuracy: 0.5827 - val_loss: 3.4089\n",
      "Epoch 472/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9555 - loss: 0.1428 - val_accuracy: 0.5908 - val_loss: 3.4762\n",
      "Epoch 473/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9601 - loss: 0.1319 - val_accuracy: 0.5724 - val_loss: 3.4582\n",
      "Epoch 474/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9508 - loss: 0.1486 - val_accuracy: 0.5816 - val_loss: 3.4517\n",
      "Epoch 475/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9525 - loss: 0.1479 - val_accuracy: 0.6000 - val_loss: 3.5224\n",
      "Epoch 476/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9484 - loss: 0.1552 - val_accuracy: 0.5939 - val_loss: 3.4555\n",
      "Epoch 477/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9601 - loss: 0.1441 - val_accuracy: 0.5796 - val_loss: 3.4887\n",
      "Epoch 478/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9676 - loss: 0.1341 - val_accuracy: 0.5765 - val_loss: 3.5598\n",
      "Epoch 479/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9321 - loss: 0.1688 - val_accuracy: 0.5949 - val_loss: 3.5577\n",
      "Epoch 480/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9620 - loss: 0.1379 - val_accuracy: 0.5888 - val_loss: 3.4836\n",
      "Epoch 481/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9571 - loss: 0.1385 - val_accuracy: 0.5939 - val_loss: 3.5034\n",
      "Epoch 482/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9545 - loss: 0.1479 - val_accuracy: 0.5796 - val_loss: 3.5427\n",
      "Epoch 483/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9608 - loss: 0.1316 - val_accuracy: 0.5816 - val_loss: 3.4931\n",
      "Epoch 484/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9529 - loss: 0.1487 - val_accuracy: 0.5878 - val_loss: 3.5147\n",
      "Epoch 485/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9632 - loss: 0.1313 - val_accuracy: 0.5714 - val_loss: 3.5226\n",
      "Epoch 486/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9542 - loss: 0.1429 - val_accuracy: 0.5939 - val_loss: 3.6094\n",
      "Epoch 487/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9519 - loss: 0.1507 - val_accuracy: 0.5684 - val_loss: 3.5575\n",
      "Epoch 488/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9453 - loss: 0.1727 - val_accuracy: 0.5898 - val_loss: 3.6480\n",
      "Epoch 489/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9508 - loss: 0.1525 - val_accuracy: 0.5806 - val_loss: 3.6350\n",
      "Epoch 490/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9561 - loss: 0.1368 - val_accuracy: 0.5929 - val_loss: 3.6103\n",
      "Epoch 491/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9627 - loss: 0.1419 - val_accuracy: 0.5929 - val_loss: 3.6645\n",
      "Epoch 492/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9758 - loss: 0.1142 - val_accuracy: 0.5827 - val_loss: 3.6465\n",
      "Epoch 493/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9498 - loss: 0.1510 - val_accuracy: 0.5796 - val_loss: 3.6117\n",
      "Epoch 494/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9624 - loss: 0.1217 - val_accuracy: 0.5816 - val_loss: 3.6297\n",
      "Epoch 495/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9600 - loss: 0.1380 - val_accuracy: 0.5755 - val_loss: 3.5742\n",
      "Epoch 496/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9514 - loss: 0.1531 - val_accuracy: 0.5980 - val_loss: 3.6745\n",
      "Epoch 497/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9598 - loss: 0.1285 - val_accuracy: 0.5888 - val_loss: 3.6755\n",
      "Epoch 498/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9547 - loss: 0.1385 - val_accuracy: 0.5796 - val_loss: 3.6551\n",
      "Epoch 499/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9603 - loss: 0.1263 - val_accuracy: 0.5990 - val_loss: 3.7079\n",
      "Epoch 500/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9537 - loss: 0.1348 - val_accuracy: 0.5806 - val_loss: 3.6239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 501/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9499 - loss: 0.1487 - val_accuracy: 0.5796 - val_loss: 3.6640\n",
      "Epoch 502/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9566 - loss: 0.1369 - val_accuracy: 0.5776 - val_loss: 3.7294\n",
      "Epoch 503/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9500 - loss: 0.1618 - val_accuracy: 0.5959 - val_loss: 3.6664\n",
      "Epoch 504/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9540 - loss: 0.1338 - val_accuracy: 0.5908 - val_loss: 3.8094\n",
      "Epoch 505/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9579 - loss: 0.1381 - val_accuracy: 0.5867 - val_loss: 3.7460\n",
      "Epoch 506/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9640 - loss: 0.1264 - val_accuracy: 0.5939 - val_loss: 3.7032\n",
      "Epoch 507/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9582 - loss: 0.1276 - val_accuracy: 0.5806 - val_loss: 3.7405\n",
      "Epoch 508/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9578 - loss: 0.1320 - val_accuracy: 0.5745 - val_loss: 3.7719\n",
      "Epoch 509/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9515 - loss: 0.1394 - val_accuracy: 0.5857 - val_loss: 3.8383\n",
      "Epoch 510/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9419 - loss: 0.1676 - val_accuracy: 0.5847 - val_loss: 3.7958\n",
      "Epoch 511/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9486 - loss: 0.1512 - val_accuracy: 0.5929 - val_loss: 3.7848\n",
      "Epoch 512/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9589 - loss: 0.1309 - val_accuracy: 0.5888 - val_loss: 3.7587\n",
      "Epoch 513/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9508 - loss: 0.1626 - val_accuracy: 0.5939 - val_loss: 3.7598\n",
      "Epoch 514/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9498 - loss: 0.1355 - val_accuracy: 0.5949 - val_loss: 3.7910\n",
      "Epoch 515/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9669 - loss: 0.1191 - val_accuracy: 0.5878 - val_loss: 3.7776\n",
      "Epoch 516/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9617 - loss: 0.1246 - val_accuracy: 0.5857 - val_loss: 3.7643\n",
      "Epoch 517/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9637 - loss: 0.1167 - val_accuracy: 0.5980 - val_loss: 3.8051\n",
      "Epoch 518/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9570 - loss: 0.1297 - val_accuracy: 0.5980 - val_loss: 3.8011\n",
      "Epoch 519/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9665 - loss: 0.1235 - val_accuracy: 0.5857 - val_loss: 3.8301\n",
      "Epoch 520/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9702 - loss: 0.1058 - val_accuracy: 0.5857 - val_loss: 3.8748\n",
      "Epoch 521/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9588 - loss: 0.1286 - val_accuracy: 0.5878 - val_loss: 3.9074\n",
      "Epoch 522/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9554 - loss: 0.1409 - val_accuracy: 0.5847 - val_loss: 3.7907\n",
      "Epoch 523/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9441 - loss: 0.1488 - val_accuracy: 0.5867 - val_loss: 3.8316\n",
      "Epoch 524/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9665 - loss: 0.1176 - val_accuracy: 0.5888 - val_loss: 3.8291\n",
      "Epoch 525/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9657 - loss: 0.1243 - val_accuracy: 0.5898 - val_loss: 3.8896\n",
      "Epoch 526/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9565 - loss: 0.1463 - val_accuracy: 0.5878 - val_loss: 3.7696\n",
      "Epoch 527/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9685 - loss: 0.1097 - val_accuracy: 0.5980 - val_loss: 3.8662\n",
      "Epoch 528/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9701 - loss: 0.1149 - val_accuracy: 0.5755 - val_loss: 3.8482\n",
      "Epoch 529/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9639 - loss: 0.1245 - val_accuracy: 0.6051 - val_loss: 3.8581\n",
      "Epoch 530/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9670 - loss: 0.1139 - val_accuracy: 0.5867 - val_loss: 3.8684\n",
      "Epoch 531/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9614 - loss: 0.1225 - val_accuracy: 0.5898 - val_loss: 3.8587\n",
      "Epoch 532/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9652 - loss: 0.1168 - val_accuracy: 0.6000 - val_loss: 3.8940\n",
      "Epoch 533/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9721 - loss: 0.1145 - val_accuracy: 0.5878 - val_loss: 3.9472\n",
      "Epoch 534/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9660 - loss: 0.1221 - val_accuracy: 0.5857 - val_loss: 4.0348\n",
      "Epoch 535/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9479 - loss: 0.1592 - val_accuracy: 0.5898 - val_loss: 3.9183\n",
      "Epoch 536/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9603 - loss: 0.1348 - val_accuracy: 0.5847 - val_loss: 3.9185\n",
      "Epoch 537/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9529 - loss: 0.1385 - val_accuracy: 0.6000 - val_loss: 3.9514\n",
      "Epoch 538/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9572 - loss: 0.1409 - val_accuracy: 0.5878 - val_loss: 3.9352\n",
      "Epoch 539/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9652 - loss: 0.1214 - val_accuracy: 0.5990 - val_loss: 3.9134\n",
      "Epoch 540/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9589 - loss: 0.1236 - val_accuracy: 0.5969 - val_loss: 3.9984\n",
      "Epoch 541/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9652 - loss: 0.1202 - val_accuracy: 0.5714 - val_loss: 4.0191\n",
      "Epoch 542/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9575 - loss: 0.1318 - val_accuracy: 0.5959 - val_loss: 3.9688\n",
      "Epoch 543/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9708 - loss: 0.1076 - val_accuracy: 0.5898 - val_loss: 3.9694\n",
      "Epoch 544/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9669 - loss: 0.1194 - val_accuracy: 0.6020 - val_loss: 3.9582\n",
      "Epoch 545/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9563 - loss: 0.1281 - val_accuracy: 0.5888 - val_loss: 4.0179\n",
      "Epoch 546/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9691 - loss: 0.1004 - val_accuracy: 0.5939 - val_loss: 4.0035\n",
      "Epoch 547/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9636 - loss: 0.1150 - val_accuracy: 0.5816 - val_loss: 3.9207\n",
      "Epoch 548/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9796 - loss: 0.0976 - val_accuracy: 0.5816 - val_loss: 3.9670\n",
      "Epoch 549/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9742 - loss: 0.0995 - val_accuracy: 0.5837 - val_loss: 4.0664\n",
      "Epoch 550/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9663 - loss: 0.1145 - val_accuracy: 0.5898 - val_loss: 4.0211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 551/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9732 - loss: 0.1062 - val_accuracy: 0.6010 - val_loss: 4.0331\n",
      "Epoch 552/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9754 - loss: 0.1127 - val_accuracy: 0.5827 - val_loss: 4.0825\n",
      "Epoch 553/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9625 - loss: 0.1258 - val_accuracy: 0.6010 - val_loss: 4.0044\n",
      "Epoch 554/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9763 - loss: 0.0951 - val_accuracy: 0.5908 - val_loss: 4.0467\n",
      "Epoch 555/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9748 - loss: 0.0947 - val_accuracy: 0.5939 - val_loss: 4.0857\n",
      "Epoch 556/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9628 - loss: 0.1190 - val_accuracy: 0.5878 - val_loss: 4.0564\n",
      "Epoch 557/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9628 - loss: 0.1210 - val_accuracy: 0.5827 - val_loss: 4.0287\n",
      "Epoch 558/1000\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9712 - loss: 0.1029 - val_accuracy: 0.5888 - val_loss: 4.0513\n",
      "Epoch 559/1000\n",
      "\u001b[1m67/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9727 - loss: 0.1041"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./model/whitewine_loss_\u001b[39m\u001b[38;5;132;01m{val_loss:.4f}\u001b[39;00m\u001b[38;5;124m_epoch_\u001b[39m\u001b[38;5;132;01m{epoch:04d}\u001b[39;00m\u001b[38;5;124m__.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m model_save \u001b[38;5;241m=\u001b[39m ModelCheckpoint(filepath\u001b[38;5;241m=\u001b[39mfilepath, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 6\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrs_X_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrs_X_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_save\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    217\u001b[0m     iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    218\u001b[0m ):\n\u001b[1;32m    219\u001b[0m     opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mget_value()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "early_stop = EarlyStopping(patience=1000)\n",
    "os.makedirs(\"./model\", exist_ok=True) #디렉토리 자동 생성\n",
    "filepath = \"./model/whitewine_loss_{val_loss:.4f}_epoch_{epoch:04d}__.keras\"\n",
    "model_save = ModelCheckpoint(filepath=filepath, save_best_only=True)\n",
    "history = model.fit(rs_X_train, y_train,\n",
    "                   epochs=1000, \n",
    "                   batch_size=32,\n",
    "                   validation_data=(rs_X_valid, y_valid),\n",
    "                   callbacks=[early_stop, model_save])\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend(['train', 'valid'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0610714",
   "metadata": {},
   "source": [
    "# 데이터 증폭 후 분석\n",
    "* 데이터 증폭 + 스케일러 사용시\n",
    "* 데이터 증폭 후 스케일러로 분석시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "731d427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3d7bd3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(\"quality\", axis=1)\n",
    "y = data['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9cc929e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 홀드아웃 진행\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size=0.5, stratify=y_valid, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "112c0398",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wichuuu/miniforge3/envs/tf/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "smt = SMOTE(k_neighbors=2, random_state=42)\n",
    "smt_X_train, smt_y_train = smt.fit_resample(X_train, y_train)\n",
    "smt_X_train = pd.DataFrame(smt_X_train, columns=X_train.columns)\n",
    "smt_y_train = pd.Series(smt_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e8f31def",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>3.160000</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>12.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.700000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>0.079000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>0.996620</td>\n",
       "      <td>3.020000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>8.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.600000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>0.991200</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>12.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>14.700000</td>\n",
       "      <td>0.051000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>0.997920</td>\n",
       "      <td>2.960000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>0.991800</td>\n",
       "      <td>3.050000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9221</th>\n",
       "      <td>7.390043</td>\n",
       "      <td>0.240664</td>\n",
       "      <td>0.364315</td>\n",
       "      <td>2.006638</td>\n",
       "      <td>0.031033</td>\n",
       "      <td>27.132757</td>\n",
       "      <td>138.137077</td>\n",
       "      <td>0.990542</td>\n",
       "      <td>3.282987</td>\n",
       "      <td>0.478009</td>\n",
       "      <td>12.513276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9222</th>\n",
       "      <td>6.751973</td>\n",
       "      <td>0.329605</td>\n",
       "      <td>0.350789</td>\n",
       "      <td>1.782368</td>\n",
       "      <td>0.024343</td>\n",
       "      <td>26.127625</td>\n",
       "      <td>93.510499</td>\n",
       "      <td>0.989848</td>\n",
       "      <td>3.397842</td>\n",
       "      <td>0.552250</td>\n",
       "      <td>12.551973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9223</th>\n",
       "      <td>6.773419</td>\n",
       "      <td>0.325316</td>\n",
       "      <td>0.359367</td>\n",
       "      <td>1.808102</td>\n",
       "      <td>0.024815</td>\n",
       "      <td>26.427860</td>\n",
       "      <td>94.711442</td>\n",
       "      <td>0.989875</td>\n",
       "      <td>3.396127</td>\n",
       "      <td>0.544101</td>\n",
       "      <td>12.573419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9224</th>\n",
       "      <td>6.630848</td>\n",
       "      <td>0.353830</td>\n",
       "      <td>0.302339</td>\n",
       "      <td>1.637017</td>\n",
       "      <td>0.021679</td>\n",
       "      <td>24.431866</td>\n",
       "      <td>86.727465</td>\n",
       "      <td>0.989690</td>\n",
       "      <td>3.407532</td>\n",
       "      <td>0.598278</td>\n",
       "      <td>12.430848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9225</th>\n",
       "      <td>7.035602</td>\n",
       "      <td>0.272880</td>\n",
       "      <td>0.464241</td>\n",
       "      <td>2.122723</td>\n",
       "      <td>0.030583</td>\n",
       "      <td>30.098431</td>\n",
       "      <td>109.393726</td>\n",
       "      <td>0.990216</td>\n",
       "      <td>3.375152</td>\n",
       "      <td>0.444471</td>\n",
       "      <td>12.835602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9226 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0          6.800000          0.210000     0.270000        2.100000   0.030000   \n",
       "1          6.700000          0.240000     0.320000       10.300000   0.079000   \n",
       "2          7.600000          0.270000     0.420000        2.600000   0.044000   \n",
       "3          7.000000          0.150000     0.280000       14.700000   0.051000   \n",
       "4          7.500000          0.170000     0.340000        1.400000   0.035000   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "9221       7.390043          0.240664     0.364315        2.006638   0.031033   \n",
       "9222       6.751973          0.329605     0.350789        1.782368   0.024343   \n",
       "9223       6.773419          0.325316     0.359367        1.808102   0.024815   \n",
       "9224       6.630848          0.353830     0.302339        1.637017   0.021679   \n",
       "9225       7.035602          0.272880     0.464241        2.122723   0.030583   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide   density        pH  \\\n",
       "0               26.000000            139.000000  0.990000  3.160000   \n",
       "1               37.000000            122.000000  0.996620  3.020000   \n",
       "2               29.000000            110.000000  0.991200  3.310000   \n",
       "3               29.000000            149.000000  0.997920  2.960000   \n",
       "4               13.000000            102.000000  0.991800  3.050000   \n",
       "...                   ...                   ...       ...       ...   \n",
       "9221            27.132757            138.137077  0.990542  3.282987   \n",
       "9222            26.127625             93.510499  0.989848  3.397842   \n",
       "9223            26.427860             94.711442  0.989875  3.396127   \n",
       "9224            24.431866             86.727465  0.989690  3.407532   \n",
       "9225            30.098431            109.393726  0.990216  3.375152   \n",
       "\n",
       "      sulphates    alcohol  \n",
       "0      0.610000  12.600000  \n",
       "1      0.450000   8.800000  \n",
       "2      0.510000  12.700000  \n",
       "3      0.390000   9.000000  \n",
       "4      0.740000  11.000000  \n",
       "...         ...        ...  \n",
       "9221   0.478009  12.513276  \n",
       "9222   0.552250  12.551973  \n",
       "9223   0.544101  12.573419  \n",
       "9224   0.598278  12.430848  \n",
       "9225   0.444471  12.835602  \n",
       "\n",
       "[9226 rows x 11 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smt_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "88782c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quality\n",
       "7    1318\n",
       "5    1318\n",
       "6    1318\n",
       "4    1318\n",
       "8    1318\n",
       "3    1318\n",
       "9    1318\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smt_y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6f03ed22",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (9226, 11), indices imply (2938, 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m valid_temp \u001b[38;5;241m=\u001b[39m rs\u001b[38;5;241m.\u001b[39mtransform(X_valid)\n\u001b[1;32m      5\u001b[0m test_temp \u001b[38;5;241m=\u001b[39m rs\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m----> 7\u001b[0m rs_X_train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_temp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m rs_X_valid \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(valid_temp, columns\u001b[38;5;241m=\u001b[39mX_valid\u001b[38;5;241m.\u001b[39mcolumns, index\u001b[38;5;241m=\u001b[39mX_valid\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m      9\u001b[0m rs_X_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(test_temp, columns\u001b[38;5;241m=\u001b[39mX_test\u001b[38;5;241m.\u001b[39mcolumns, index\u001b[38;5;241m=\u001b[39mX_test\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py:827\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    816\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    817\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[1;32m    818\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    824\u001b[0m             copy\u001b[38;5;241m=\u001b[39m_copy,\n\u001b[1;32m    825\u001b[0m         )\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 827\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py:336\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[1;32m    332\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[1;32m    333\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[1;32m    334\u001b[0m )\n\u001b[0;32m--> 336\u001b[0m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/pandas/core/internals/construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[0;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (9226, 11), indices imply (2938, 11)"
     ]
    }
   ],
   "source": [
    "# 스케일링\n",
    "rs = RobustScaler()\n",
    "train_temp = rs.fit_transform(smt_X_train)\n",
    "valid_temp = rs.transform(X_valid)\n",
    "test_temp = rs.transform(X_test)\n",
    "\n",
    "rs_X_train = pd.DataFrame(train_temp, columns=X_train.columns, index=X_train.index)\n",
    "rs_X_valid = pd.DataFrame(valid_temp, columns=X_valid.columns, index=X_valid.index)\n",
    "rs_X_test = pd.DataFrame(test_temp, columns=X_test.columns, index=X_test.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ed31f481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9221</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9222</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9223</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9224</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9225</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9226 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          3      4      5      6      7      8      9\n",
       "0     False  False  False  False   True  False  False\n",
       "1     False  False   True  False  False  False  False\n",
       "2     False  False  False   True  False  False  False\n",
       "3     False  False  False  False   True  False  False\n",
       "4     False  False   True  False  False  False  False\n",
       "...     ...    ...    ...    ...    ...    ...    ...\n",
       "9221  False  False  False  False  False  False   True\n",
       "9222  False  False  False  False  False  False   True\n",
       "9223  False  False  False  False  False  False   True\n",
       "9224  False  False  False  False  False  False   True\n",
       "9225  False  False  False  False  False  False   True\n",
       "\n",
       "[9226 rows x 7 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ot_smt_y_train = pd.get_dummies(smt_y_train)\n",
    "ot_smt_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "20f6efc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2613</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2361</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4245</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3947</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3799</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4502</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3148</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>980 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          3      4      5      6      7      8      9\n",
       "2613  False  False   True  False  False  False  False\n",
       "2361  False  False   True  False  False  False  False\n",
       "4245  False  False   True  False  False  False  False\n",
       "3947  False  False  False  False   True  False  False\n",
       "3799  False  False   True  False  False  False  False\n",
       "...     ...    ...    ...    ...    ...    ...    ...\n",
       "4502  False  False  False   True  False  False  False\n",
       "3148  False  False  False   True  False  False  False\n",
       "1562  False  False  False  False   True  False  False\n",
       "37    False  False  False   True  False  False  False\n",
       "668   False  False  False   True  False  False  False\n",
       "\n",
       "[980 rows x 7 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = pd.get_dummies(y_test)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c47feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = ot_smt_y_train.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(smt_X_train.shape[1],)))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c14a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "early_stop = EarlyStopping(patience=1000)\n",
    "os.makedirs(\"./model\", exist_ok=True) #디렉토리 자동 생성\n",
    "filepath = \"./model/whitewine_loss_{val_loss:.4f}_epoch_{epoch:04d}__.keras\"\n",
    "model_save = ModelCheckpoint(filepath=filepath, save_best_only=True)\n",
    "history = model.fit(rs_X_train, y_train,\n",
    "                   epochs=1000, \n",
    "                   batch_size=32,\n",
    "                   validation_data=(rs_X_valid, y_valid),\n",
    "                   callbacks=[early_stop, model_save])\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend(['train', 'valid'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5373ec12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fdacf15",
   "metadata": {},
   "source": [
    "# 저장된 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6fa40310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>0.454545</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0.280488</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.622478</td>\n",
       "      <td>1.30</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-0.421053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.109756</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.945245</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.285714</td>\n",
       "      <td>-0.736842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2886</th>\n",
       "      <td>-0.181818</td>\n",
       "      <td>1.363636</td>\n",
       "      <td>-0.083333</td>\n",
       "      <td>-0.439024</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>-0.727273</td>\n",
       "      <td>-0.566667</td>\n",
       "      <td>-0.426513</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3590</th>\n",
       "      <td>-0.181818</td>\n",
       "      <td>-0.636364</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.451220</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.862248</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>0.842105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4127</th>\n",
       "      <td>-0.090909</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.364265</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>-0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3178</th>\n",
       "      <td>0.727273</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.109756</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.616667</td>\n",
       "      <td>0.096830</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.285714</td>\n",
       "      <td>-0.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>0.818182</td>\n",
       "      <td>-0.181818</td>\n",
       "      <td>-0.416667</td>\n",
       "      <td>-0.414634</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>-0.230548</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2102</th>\n",
       "      <td>-0.545455</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.719512</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0.991354</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>-0.894737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.542683</td>\n",
       "      <td>-1.285714</td>\n",
       "      <td>-0.363636</td>\n",
       "      <td>-0.233333</td>\n",
       "      <td>-0.299712</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>0.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3774</th>\n",
       "      <td>-0.090909</td>\n",
       "      <td>1.454545</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.841463</td>\n",
       "      <td>-0.214286</td>\n",
       "      <td>1.227273</td>\n",
       "      <td>1.883333</td>\n",
       "      <td>0.954467</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>-0.368421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>980 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "606        0.454545         -0.090909    -0.250000        0.280488   0.428571   \n",
       "235        0.363636         -0.272727     0.500000        1.109756   1.071429   \n",
       "2886      -0.181818          1.363636    -0.083333       -0.439024  -0.071429   \n",
       "3590      -0.181818         -0.636364     0.250000       -0.451220  -0.428571   \n",
       "4127      -0.090909          0.454545     0.166667        0.195122   1.142857   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "3178       0.727273         -1.000000     0.250000       -0.109756   0.571429   \n",
       "618        0.818182         -0.181818    -0.416667       -0.414634   0.571429   \n",
       "2102      -0.545455          0.727273    -0.166667        0.719512   0.285714   \n",
       "2695      -1.000000         -0.272727    -0.333333        0.542683  -1.285714   \n",
       "3774      -0.090909          1.454545     0.583333        0.841463  -0.214286   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide   density    pH  sulphates  \\\n",
       "606              0.181818              0.383333  0.622478  1.30  -0.714286   \n",
       "235              0.954545              0.983333  0.945245 -0.40  -0.285714   \n",
       "2886            -0.727273             -0.566667 -0.426513 -0.20  -0.500000   \n",
       "3590             0.136364             -0.466667 -0.862248  0.05   1.428571   \n",
       "4127             0.772727              1.333333  0.364265  0.80   0.571429   \n",
       "...                   ...                   ...       ...   ...        ...   \n",
       "3178            -0.500000             -0.616667  0.096830 -0.35  -0.285714   \n",
       "618              0.818182              0.916667 -0.230548  0.30   0.142857   \n",
       "2102            -0.272727              1.700000  0.991354  0.05   0.071429   \n",
       "2695            -0.363636             -0.233333 -0.299712  0.55  -0.714286   \n",
       "3774             1.227273              1.883333  0.954467  0.70   0.714286   \n",
       "\n",
       "       alcohol  \n",
       "606  -0.421053  \n",
       "235  -0.736842  \n",
       "2886  0.052632  \n",
       "3590  0.842105  \n",
       "4127 -0.052632  \n",
       "...        ...  \n",
       "3178 -0.473684  \n",
       "618   0.210526  \n",
       "2102 -0.894737  \n",
       "2695  0.473684  \n",
       "3774 -0.368421  \n",
       "\n",
       "[980 rows x 11 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "289b83c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fc4d221d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009681</td>\n",
       "      <td>0.024053</td>\n",
       "      <td>0.375952</td>\n",
       "      <td>0.527552</td>\n",
       "      <td>0.053483</td>\n",
       "      <td>0.008913</td>\n",
       "      <td>0.000364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006830</td>\n",
       "      <td>0.006402</td>\n",
       "      <td>0.336747</td>\n",
       "      <td>0.462587</td>\n",
       "      <td>0.164633</td>\n",
       "      <td>0.022681</td>\n",
       "      <td>0.000120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.010873</td>\n",
       "      <td>0.266340</td>\n",
       "      <td>0.386448</td>\n",
       "      <td>0.317700</td>\n",
       "      <td>0.016702</td>\n",
       "      <td>0.001463</td>\n",
       "      <td>0.000474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.008845</td>\n",
       "      <td>0.382938</td>\n",
       "      <td>0.563065</td>\n",
       "      <td>0.044133</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.037898</td>\n",
       "      <td>0.431740</td>\n",
       "      <td>0.433991</td>\n",
       "      <td>0.039587</td>\n",
       "      <td>0.013869</td>\n",
       "      <td>0.001206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>0.002544</td>\n",
       "      <td>0.004889</td>\n",
       "      <td>0.199648</td>\n",
       "      <td>0.512240</td>\n",
       "      <td>0.260310</td>\n",
       "      <td>0.020220</td>\n",
       "      <td>0.000149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.003937</td>\n",
       "      <td>0.191166</td>\n",
       "      <td>0.740109</td>\n",
       "      <td>0.039256</td>\n",
       "      <td>0.010252</td>\n",
       "      <td>0.000243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>0.003274</td>\n",
       "      <td>0.007008</td>\n",
       "      <td>0.683655</td>\n",
       "      <td>0.287388</td>\n",
       "      <td>0.018037</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.002288</td>\n",
       "      <td>0.025767</td>\n",
       "      <td>0.524091</td>\n",
       "      <td>0.319313</td>\n",
       "      <td>0.127078</td>\n",
       "      <td>0.001273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>0.009385</td>\n",
       "      <td>0.043018</td>\n",
       "      <td>0.327793</td>\n",
       "      <td>0.598960</td>\n",
       "      <td>0.018051</td>\n",
       "      <td>0.002656</td>\n",
       "      <td>0.000137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>980 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            3         4         5         6         7         8         9\n",
       "0    0.009681  0.024053  0.375952  0.527552  0.053483  0.008913  0.000364\n",
       "1    0.006830  0.006402  0.336747  0.462587  0.164633  0.022681  0.000120\n",
       "2    0.010873  0.266340  0.386448  0.317700  0.016702  0.001463  0.000474\n",
       "3    0.000138  0.000506  0.008845  0.382938  0.563065  0.044133  0.000374\n",
       "4    0.041708  0.037898  0.431740  0.433991  0.039587  0.013869  0.001206\n",
       "..        ...       ...       ...       ...       ...       ...       ...\n",
       "975  0.002544  0.004889  0.199648  0.512240  0.260310  0.020220  0.000149\n",
       "976  0.015038  0.003937  0.191166  0.740109  0.039256  0.010252  0.000243\n",
       "977  0.003274  0.007008  0.683655  0.287388  0.018037  0.000628  0.000010\n",
       "978  0.000191  0.002288  0.025767  0.524091  0.319313  0.127078  0.001273\n",
       "979  0.009385  0.043018  0.327793  0.598960  0.018051  0.002656  0.000137\n",
       "\n",
       "[980 rows x 7 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = load_model(\"./model/whitewine_loss_1.0566_epoch_0022__.keras\")\n",
    "wine_pred = best_model.predict(rs_X_test)\n",
    "wine_pred = pd.DataFrame(wine_pred, columns=y_test.columns)\n",
    "wine_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cb5234d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.3492 - loss: 1.9421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8633756637573242, 0.3622449040412903]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.evaluate(rs_X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf87d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83420a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
